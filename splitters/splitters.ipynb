{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWjnuB0m0C97N2i/Z1sUPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakarkhanlakhwera/Langchain/blob/main/splitters/splitters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mKkqg6Unmrtz",
        "outputId": "69b4d85e-3378-41b6-aa90-1e050fa09ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.18 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 pypdf-5.3.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://nlp-weekly-update.hashnode.dev/weekly-deep-learning-focus-mastering-the-essentials\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be9zFFUApvoM",
        "outputId": "1585f011-8aa1-4fb4-f54f-3854baf4ee83"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "zvfzQBN3qT1t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBBlk5UEyFAt",
        "outputId": "66cde440-34da-4ab5-a16e-29cd4834ec77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J-eR9YEWqX7q",
        "outputId": "f6e217dd-62d6-4c2f-c07d-15378b910bb1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://nlp-weekly-update.hashnode.dev/weekly-deep-learning-focus-mastering-the-essentials', 'title': 'Weekly Deep Learning Focus: Mastering the Essentials', 'description': 'Introduction\\nEmbarking on a deep learning journey can be overwhelming without a structured approach. To tackle this challenge, I’ve planned an entire week dedicated to mastering key concepts in deep learning. In this article, I’ll outline my weekly l...', 'language': 'en'}, page_content='Weekly Deep Learning Focus: Mastering the EssentialsNLP-Weekly-updateFollowNLP-Weekly-updateFollowWeekly Deep Learning Focus: Mastering the EssentialsAbuBakar khan lakhwera·Jan 5, 2025·3 min readIntroduction\\nEmbarking on a deep learning journey can be overwhelming without a structured approach. To tackle this challenge, I’ve planned an entire week dedicated to mastering key concepts in deep learning. In this article, I’ll outline my weekly learning plan, including daily goals, practical tasks, and expected outcomes. Whether you’re a beginner or looking to revisit the fundamentals, this roadmap might inspire your own deep learning journey!\\n\\nDay 1: Optimization in Deep Learning\\nOptimization lies at the heart of neural network training. On Day 1, I’ll focus on understanding:\\n\\nGradient descent and its variations.\\n\\nLearning rate schedules and their impact.\\n\\nOptimizers like Adam, RMSProp, and Stochastic Gradient Descent (SGD).\\n\\n\\nPractical Task:\\n\\nImplement gradient descent manually for a simple linear regression model.\\n\\nCompare the performance of manual gradient descent with built-in optimizers in PyTorch or TensorFlow.\\n\\n\\n\\nDay 2: Activation Functions and Their Impact\\nActivation functions add non-linearity, enabling neural networks to learn complex patterns. Today’s learning will include:\\n\\nSigmoid, ReLU, Leaky ReLU, Tanh, and Softmax functions.\\n\\nWhen and why to use specific activation functions.\\n\\n\\nPractical Task:\\n\\nBuild a small MLP and experiment with different activation functions to observe their impact on training speed and convergence.\\n\\n\\nDay 3: Loss Functions and Regularization\\nUnderstanding loss functions and regularization is essential to improving model performance. My focus areas:\\n\\nLoss functions: Mean Squared Error (MSE), Cross-Entropy, and Hinge Loss.\\n\\nRegularization techniques: L1/L2 regularization, dropout, and batch normalization.\\n\\n\\nPractical Task:\\n\\nTrain a model on a small dataset and address overfitting by implementing regularization techniques.\\n\\n\\nDay 4: Data Preprocessing and Augmentation\\nData preprocessing and augmentation are critical for feeding clean, meaningful data into neural networks. Key learnings:\\n\\nPreprocessing techniques: normalization, standardization, and handling missing data.\\n\\nData augmentation techniques: flipping, rotation, scaling, and more.\\n\\n\\nPractical Task:\\n\\nChoose an open dataset, preprocess it, and apply data augmentation techniques to prepare it for training.\\n\\n\\nDay 5: Introduction to Convolutional Neural Networks (CNNs)\\nCNNs revolutionized deep learning for image-based tasks. On Day 5, I’ll explore:\\n\\nHow convolutional layers work for feature extraction.\\n\\nPooling layers and their role in dimensionality reduction.\\n\\n\\nPractical Task:\\n\\nBuild a basic CNN for image classification using the MNIST or CIFAR-10 dataset.\\n\\n\\nDay 6: Hyperparameter Tuning\\nHyperparameter tuning can significantly improve model performance. Today’s topics include:\\n\\nAdjusting learning rate, batch size, epochs, and network architecture.\\n\\nUsing tools like Grid Search and Random Search.\\n\\n\\nPractical Task:\\n\\nExperiment with hyperparameter tuning on a small dataset and evaluate its impact on model accuracy.\\n\\n\\nDay 7: Review and Reflect\\nThe final day is reserved for consolidating my learnings:\\n\\nRevisit the key concepts covered throughout the week.\\n\\nSummarize learnings in a blog post to reinforce understanding and share insights.\\n\\n\\nPractical Task:\\n\\nWrite and publish a Hashnode article documenting the week’s journey and outcomes.\\n\\n\\nWhy This Plan Works\\nThis plan is structured to balance theory and practice, ensuring consistent progress. It emphasizes hands-on projects to solidify concepts and provides opportunities to reflect and learn from mistakes.\\nJoin the Journey\\nAre you on a similar deep learning path? Let’s connect, share ideas, and grow together in this exciting field of AI. I’d love to hear your thoughts, feedback, or even how you’re planning your own learning journey!\\nDeep Learning\\xa0Share this')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "2ybe2DuWqZBs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 300\n",
        "chunk_overlap = 50\n",
        "r_text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "c_text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
      ],
      "metadata": {
        "id": "KyEuR5tEt9TJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NwYr35mkyyt0",
        "outputId": "42a2dbf2-d042-4922-d664-549a8e067525"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://nlp-weekly-update.hashnode.dev/weekly-deep-learning-focus-mastering-the-essentials', 'title': 'Weekly Deep Learning Focus: Mastering the Essentials', 'description': 'Introduction\\nEmbarking on a deep learning journey can be overwhelming without a structured approach. To tackle this challenge, I’ve planned an entire week dedicated to mastering key concepts in deep learning. In this article, I’ll outline my weekly l...', 'language': 'en'}, page_content='Weekly Deep Learning Focus: Mastering the EssentialsNLP-Weekly-updateFollowNLP-Weekly-updateFollowWeekly Deep Learning Focus: Mastering the EssentialsAbuBakar khan lakhwera·Jan 5, 2025·3 min readIntroduction\\nEmbarking on a deep learning journey can be overwhelming without a structured approach. To tackle this challenge, I’ve planned an entire week dedicated to mastering key concepts in deep learning. In this article, I’ll outline my weekly learning plan, including daily goals, practical tasks, and expected outcomes. Whether you’re a beginner or looking to revisit the fundamentals, this roadmap might inspire your own deep learning journey!\\n\\nDay 1: Optimization in Deep Learning\\nOptimization lies at the heart of neural network training. On Day 1, I’ll focus on understanding:\\n\\nGradient descent and its variations.\\n\\nLearning rate schedules and their impact.\\n\\nOptimizers like Adam, RMSProp, and Stochastic Gradient Descent (SGD).\\n\\n\\nPractical Task:\\n\\nImplement gradient descent manually for a simple linear regression model.\\n\\nCompare the performance of manual gradient descent with built-in optimizers in PyTorch or TensorFlow.\\n\\n\\n\\nDay 2: Activation Functions and Their Impact\\nActivation functions add non-linearity, enabling neural networks to learn complex patterns. Today’s learning will include:\\n\\nSigmoid, ReLU, Leaky ReLU, Tanh, and Softmax functions.\\n\\nWhen and why to use specific activation functions.\\n\\n\\nPractical Task:\\n\\nBuild a small MLP and experiment with different activation functions to observe their impact on training speed and convergence.\\n\\n\\nDay 3: Loss Functions and Regularization\\nUnderstanding loss functions and regularization is essential to improving model performance. My focus areas:\\n\\nLoss functions: Mean Squared Error (MSE), Cross-Entropy, and Hinge Loss.\\n\\nRegularization techniques: L1/L2 regularization, dropout, and batch normalization.\\n\\n\\nPractical Task:\\n\\nTrain a model on a small dataset and address overfitting by implementing regularization techniques.\\n\\n\\nDay 4: Data Preprocessing and Augmentation\\nData preprocessing and augmentation are critical for feeding clean, meaningful data into neural networks. Key learnings:\\n\\nPreprocessing techniques: normalization, standardization, and handling missing data.\\n\\nData augmentation techniques: flipping, rotation, scaling, and more.\\n\\n\\nPractical Task:\\n\\nChoose an open dataset, preprocess it, and apply data augmentation techniques to prepare it for training.\\n\\n\\nDay 5: Introduction to Convolutional Neural Networks (CNNs)\\nCNNs revolutionized deep learning for image-based tasks. On Day 5, I’ll explore:\\n\\nHow convolutional layers work for feature extraction.\\n\\nPooling layers and their role in dimensionality reduction.\\n\\n\\nPractical Task:\\n\\nBuild a basic CNN for image classification using the MNIST or CIFAR-10 dataset.\\n\\n\\nDay 6: Hyperparameter Tuning\\nHyperparameter tuning can significantly improve model performance. Today’s topics include:\\n\\nAdjusting learning rate, batch size, epochs, and network architecture.\\n\\nUsing tools like Grid Search and Random Search.\\n\\n\\nPractical Task:\\n\\nExperiment with hyperparameter tuning on a small dataset and evaluate its impact on model accuracy.\\n\\n\\nDay 7: Review and Reflect\\nThe final day is reserved for consolidating my learnings:\\n\\nRevisit the key concepts covered throughout the week.\\n\\nSummarize learnings in a blog post to reinforce understanding and share insights.\\n\\n\\nPractical Task:\\n\\nWrite and publish a Hashnode article documenting the week’s journey and outcomes.\\n\\n\\nWhy This Plan Works\\nThis plan is structured to balance theory and practice, ensuring consistent progress. It emphasizes hands-on projects to solidify concepts and provides opportunities to reflect and learn from mistakes.\\nJoin the Journey\\nAre you on a similar deep learning path? Let’s connect, share ideas, and grow together in this exciting field of AI. I’d love to hear your thoughts, feedback, or even how you’re planning your own learning journey!\\nDeep Learning\\xa0Share this')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if docs:\n",
        "  for doc in docs:\n",
        "    r_docs = r_text_splitter.split_documents([doc])\n",
        "\n",
        "len(r_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx1svfT7x4Wa",
        "outputId": "f378cda4-fdc2-4c2e-a6f9-811f40c9e35c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for content in r_docs:\n",
        "  print(content.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c2gRRKmgx90k",
        "outputId": "b2ca6ebc-e929-4d80-920b-5b82b856007f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekly Deep Learning Focus: Mastering the EssentialsNLP-Weekly-updateFollowNLP-Weekly-updateFollowWeekly Deep Learning Focus: Mastering the EssentialsAbuBakar khan lakhwera·Jan 5, 2025·3 min readIntroduction\n",
            "Embarking on a deep learning journey can be overwhelming without a structured approach. To tackle this challenge, I’ve planned an entire week dedicated to mastering key concepts in deep learning. In this article, I’ll outline my weekly learning plan, including daily goals, practical tasks, and\n",
            "plan, including daily goals, practical tasks, and expected outcomes. Whether you’re a beginner or looking to revisit the fundamentals, this roadmap might inspire your own deep learning journey!\n",
            "Day 1: Optimization in Deep Learning\n",
            "Optimization lies at the heart of neural network training. On Day 1, I’ll focus on understanding:\n",
            "\n",
            "Gradient descent and its variations.\n",
            "\n",
            "Learning rate schedules and their impact.\n",
            "\n",
            "Optimizers like Adam, RMSProp, and Stochastic Gradient Descent (SGD).\n",
            "Practical Task:\n",
            "\n",
            "Implement gradient descent manually for a simple linear regression model.\n",
            "\n",
            "Compare the performance of manual gradient descent with built-in optimizers in PyTorch or TensorFlow.\n",
            "Day 2: Activation Functions and Their Impact\n",
            "Activation functions add non-linearity, enabling neural networks to learn complex patterns. Today’s learning will include:\n",
            "\n",
            "Sigmoid, ReLU, Leaky ReLU, Tanh, and Softmax functions.\n",
            "\n",
            "When and why to use specific activation functions.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "Practical Task:\n",
            "\n",
            "Build a small MLP and experiment with different activation functions to observe their impact on training speed and convergence.\n",
            "\n",
            "\n",
            "Day 3: Loss Functions and Regularization\n",
            "Understanding loss functions and regularization is essential to improving model performance. My focus areas:\n",
            "Loss functions: Mean Squared Error (MSE), Cross-Entropy, and Hinge Loss.\n",
            "\n",
            "Regularization techniques: L1/L2 regularization, dropout, and batch normalization.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Train a model on a small dataset and address overfitting by implementing regularization techniques.\n",
            "Day 4: Data Preprocessing and Augmentation\n",
            "Data preprocessing and augmentation are critical for feeding clean, meaningful data into neural networks. Key learnings:\n",
            "\n",
            "Preprocessing techniques: normalization, standardization, and handling missing data.\n",
            "Data augmentation techniques: flipping, rotation, scaling, and more.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Choose an open dataset, preprocess it, and apply data augmentation techniques to prepare it for training.\n",
            "Day 5: Introduction to Convolutional Neural Networks (CNNs)\n",
            "CNNs revolutionized deep learning for image-based tasks. On Day 5, I’ll explore:\n",
            "\n",
            "How convolutional layers work for feature extraction.\n",
            "\n",
            "Pooling layers and their role in dimensionality reduction.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "Practical Task:\n",
            "\n",
            "Build a basic CNN for image classification using the MNIST or CIFAR-10 dataset.\n",
            "\n",
            "\n",
            "Day 6: Hyperparameter Tuning\n",
            "Hyperparameter tuning can significantly improve model performance. Today’s topics include:\n",
            "\n",
            "Adjusting learning rate, batch size, epochs, and network architecture.\n",
            "Using tools like Grid Search and Random Search.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Experiment with hyperparameter tuning on a small dataset and evaluate its impact on model accuracy.\n",
            "\n",
            "\n",
            "Day 7: Review and Reflect\n",
            "The final day is reserved for consolidating my learnings:\n",
            "Revisit the key concepts covered throughout the week.\n",
            "\n",
            "Summarize learnings in a blog post to reinforce understanding and share insights.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Write and publish a Hashnode article documenting the week’s journey and outcomes.\n",
            "Why This Plan Works\n",
            "This plan is structured to balance theory and practice, ensuring consistent progress. It emphasizes hands-on projects to solidify concepts and provides opportunities to reflect and learn from mistakes.\n",
            "Join the Journey\n",
            "Join the Journey\n",
            "Are you on a similar deep learning path? Let’s connect, share ideas, and grow together in this exciting field of AI. I’d love to hear your thoughts, feedback, or even how you’re planning your own learning journey!\n",
            "Deep Learning Share this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if docs:\n",
        "  for doc in docs:\n",
        "    c_docs = c_text_splitter.split_documents([doc])\n",
        "\n",
        "len(c_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgEE9x2R0NcU",
        "outputId": "38eed254-dca6-4acf-9b8e-2800a2e64392"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 646, which is longer than the specified 300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for content in c_docs:\n",
        "  print(content.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HkinTPvq0be0",
        "outputId": "a48b495c-e736-4b46-b965-cc8079b6f173"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekly Deep Learning Focus: Mastering the EssentialsNLP-Weekly-updateFollowNLP-Weekly-updateFollowWeekly Deep Learning Focus: Mastering the EssentialsAbuBakar khan lakhwera·Jan 5, 2025·3 min readIntroduction\n",
            "Embarking on a deep learning journey can be overwhelming without a structured approach. To tackle this challenge, I’ve planned an entire week dedicated to mastering key concepts in deep learning. In this article, I’ll outline my weekly learning plan, including daily goals, practical tasks, and expected outcomes. Whether you’re a beginner or looking to revisit the fundamentals, this roadmap might inspire your own deep learning journey!\n",
            "Day 1: Optimization in Deep Learning\n",
            "Optimization lies at the heart of neural network training. On Day 1, I’ll focus on understanding:\n",
            "\n",
            "Gradient descent and its variations.\n",
            "\n",
            "Learning rate schedules and their impact.\n",
            "\n",
            "Optimizers like Adam, RMSProp, and Stochastic Gradient Descent (SGD).\n",
            "Practical Task:\n",
            "\n",
            "Implement gradient descent manually for a simple linear regression model.\n",
            "\n",
            "Compare the performance of manual gradient descent with built-in optimizers in PyTorch or TensorFlow.\n",
            "Day 2: Activation Functions and Their Impact\n",
            "Activation functions add non-linearity, enabling neural networks to learn complex patterns. Today’s learning will include:\n",
            "\n",
            "Sigmoid, ReLU, Leaky ReLU, Tanh, and Softmax functions.\n",
            "\n",
            "When and why to use specific activation functions.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "Practical Task:\n",
            "\n",
            "Build a small MLP and experiment with different activation functions to observe their impact on training speed and convergence.\n",
            "\n",
            "\n",
            "Day 3: Loss Functions and Regularization\n",
            "Understanding loss functions and regularization is essential to improving model performance. My focus areas:\n",
            "Loss functions: Mean Squared Error (MSE), Cross-Entropy, and Hinge Loss.\n",
            "\n",
            "Regularization techniques: L1/L2 regularization, dropout, and batch normalization.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Train a model on a small dataset and address overfitting by implementing regularization techniques.\n",
            "Day 4: Data Preprocessing and Augmentation\n",
            "Data preprocessing and augmentation are critical for feeding clean, meaningful data into neural networks. Key learnings:\n",
            "\n",
            "Preprocessing techniques: normalization, standardization, and handling missing data.\n",
            "Data augmentation techniques: flipping, rotation, scaling, and more.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Choose an open dataset, preprocess it, and apply data augmentation techniques to prepare it for training.\n",
            "Day 5: Introduction to Convolutional Neural Networks (CNNs)\n",
            "CNNs revolutionized deep learning for image-based tasks. On Day 5, I’ll explore:\n",
            "\n",
            "How convolutional layers work for feature extraction.\n",
            "\n",
            "Pooling layers and their role in dimensionality reduction.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "Practical Task:\n",
            "\n",
            "Build a basic CNN for image classification using the MNIST or CIFAR-10 dataset.\n",
            "\n",
            "\n",
            "Day 6: Hyperparameter Tuning\n",
            "Hyperparameter tuning can significantly improve model performance. Today’s topics include:\n",
            "\n",
            "Adjusting learning rate, batch size, epochs, and network architecture.\n",
            "Using tools like Grid Search and Random Search.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Experiment with hyperparameter tuning on a small dataset and evaluate its impact on model accuracy.\n",
            "\n",
            "\n",
            "Day 7: Review and Reflect\n",
            "The final day is reserved for consolidating my learnings:\n",
            "Revisit the key concepts covered throughout the week.\n",
            "\n",
            "Summarize learnings in a blog post to reinforce understanding and share insights.\n",
            "\n",
            "\n",
            "Practical Task:\n",
            "\n",
            "Write and publish a Hashnode article documenting the week’s journey and outcomes.\n",
            "Why This Plan Works\n",
            "This plan is structured to balance theory and practice, ensuring consistent progress. It emphasizes hands-on projects to solidify concepts and provides opportunities to reflect and learn from mistakes.\n",
            "Join the Journey\n",
            "Are you on a similar deep learning path? Let’s connect, share ideas, and grow together in this exciting field of AI. I’d love to hear your thoughts, feedback, or even how you’re planning your own learning journey!\n",
            "Deep Learning Share this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58c0tvfm0guD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}