{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QE1NPSFoAciJvekyZfsmBsJJl8mixH6E",
      "authorship_tag": "ABX9TyOpwF4GMzmJHtwOOdj9+vKL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakarkhanlakhwera/Langchain/blob/main/document_loader/document_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip   install langchain langchain_groq pypdf langchain_community fastembed \"langchain-chroma>=0.1.2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JPti-qPPow5l",
        "outputId": "58db5ab6-9ce1-4652-b4df-7a5ca2bd59c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: fastembed in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: langchain-chroma>=0.1.2 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.18.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed) (0.28.1)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from fastembed) (0.7.3)\n",
            "Requirement already satisfied: mmh3<6.0.0,>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from fastembed) (5.1.0)\n",
            "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from fastembed) (1.20.1)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from fastembed) (11.1.0)\n",
            "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from fastembed) (0.1.5)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed) (0.21.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.11/dist-packages (from fastembed) (4.67.1)\n",
            "Requirement already satisfied: chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma>=0.1.2) (0.6.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.115.8)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.34.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (3.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (4.12.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.30.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.70.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (32.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (13.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.45.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.9)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.68.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.51b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (14.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma>=0.1.2) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader('/content/drive/MyDrive/kent repertory.pdf')\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "psYNs7SQqEmd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clBd6eeCsDRd",
        "outputId": "785c7623-2b8d-4287-c62c-0b1b50e0ae0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "686"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0].metadata['source']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ohShJbRTsGEq",
        "outputId": "dd59905b-ebfd-4db5-bb8c-cac82140cc59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/kent repertory.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_n6t651bsTqq",
        "outputId": "63163fd7-2cb2-417a-f4aa-a78a90e0d52f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import Docx2txtLoader\n",
        "loader = Docx2txtLoader('/content/file-sample_100kB.docx')\n",
        "docx = loader.load()"
      ],
      "metadata": {
        "id": "mdoblX4iu4w4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc35K5miwviX",
        "outputId": "e4c5cc33-8098-4819-89de-ed40969d6a67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docx[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "collapsed": true,
        "id": "qeYSSVkzw5M4",
        "outputId": "e4e6366e-9b50-4624-a3b4-deee0a27e1a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lorem ipsum \\n\\n\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc ac faucibus odio. \\n\\n\\n\\nVestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Praesent ut varius sem. Nullam at porttitor arcu, nec lacinia nisi. Ut ac dolor vitae odio interdum condimentum. Vivamus dapibus sodales ex, vitae malesuada ipsum cursus convallis. Maecenas sed egestas nulla, ac condimentum orci. Mauris diam felis, vulputate ac suscipit et, iaculis non est. Curabitur semper arcu ac ligula semper, nec luctus nisl blandit. Integer lacinia ante ac libero lobortis imperdiet. Nullam mollis convallis ipsum, ac accumsan nunc vehicula vitae. Nulla eget justo in felis tristique fringilla. Morbi sit amet tortor quis risus auctor condimentum. Morbi in ullamcorper elit. Nulla iaculis tellus sit amet mauris tempus fringilla.\\n\\nMaecenas mauris lectus, lobortis et purus mattis, blandit dictum tellus.\\n\\nMaecenas non lorem quis tellus placerat varius. \\n\\nNulla facilisi. \\n\\nAenean congue fringilla justo ut aliquam. \\n\\nMauris id ex erat. Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. \\n\\nMorbi viverra semper lorem nec molestie. \\n\\nMaecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn non mauris justo. Duis vehicula mi vel mi pretium, a viverra erat efficitur. Cras aliquam est ac eros varius, id iaculis dui auctor. Duis pretium neque ligula, et pulvinar mi placerat et. Nulla nec nunc sit amet nunc posuere vestibulum. Ut id neque eget tortor mattis tristique. Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Cras id eros sed enim aliquam lobortis. Sed lobortis nisl ut eros efficitur tincidunt. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Ut facilisis et lacus eu cursus.\\n\\nIn eleifend velit vitae libero sollicitudin euismod. Fusce vitae vestibulum velit. Pellentesque vulputate lectus quis pellentesque commodo. Aliquam erat volutpat. Vestibulum in egestas velit. Pellentesque fermentum nisl vitae fringilla venenatis. Etiam id mauris vitae orci maximus ultricies. \\n\\n\\n\\nCras fringilla ipsum magna, in fringilla dui commodo a.\\n\\n\\n\\n\\n\\nLorem ipsum\\n\\nLorem ipsum\\n\\nLorem ipsum\\n\\n1\\n\\nIn eleifend velit vitae libero sollicitudin euismod.\\n\\nLorem\\n\\n\\n\\n2\\n\\nCras fringilla ipsum magna, in fringilla dui commodo a.\\n\\nIpsum\\n\\n\\n\\n3\\n\\nAliquam erat volutpat. \\n\\nLorem\\n\\n\\n\\n4\\n\\nFusce vitae vestibulum velit. \\n\\nLorem\\n\\n\\n\\n5\\n\\nEtiam vehicula luctus fermentum.\\n\\nIpsum\\n\\n\\n\\n\\n\\nEtiam vehicula luctus fermentum. In vel metus congue, pulvinar lectus vel, fermentum dui. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Aliquam erat volutpat. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Ut ullamcorper justo sapien, in cursus libero viverra eget. Vivamus auctor imperdiet urna, at pulvinar leo posuere laoreet. Suspendisse neque nisl, fringilla at iaculis scelerisque, ornare vel dolor. Ut et pulvinar nunc. Pellentesque fringilla mollis efficitur. Nullam venenatis commodo imperdiet. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.\\n\\n\\n\\n\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \\n\\n\\n\\nNunc ac faucibus odio. Vestibulum neque massa, scelerisque sit amet ligula eu, congue molestie mi. Praesent ut varius sem. Nullam at porttitor arcu, nec lacinia nisi. Ut ac dolor vitae odio interdum condimentum. Vivamus dapibus sodales ex, vitae malesuada ipsum cursus convallis. Maecenas sed egestas nulla, ac condimentum orci. Mauris diam felis, vulputate ac suscipit et, iaculis non est. Curabitur semper arcu ac ligula semper, nec luctus nisl blandit. Integer lacinia ante ac libero lobortis imperdiet. Nullam mollis convallis ipsum, ac accumsan nunc vehicula vitae. Nulla eget justo in felis tristique fringilla. Morbi sit amet tortor quis risus auctor condimentum. Morbi in ullamcorper elit. Nulla iaculis tellus sit amet mauris tempus fringilla.\\n\\nMaecenas mauris lectus, lobortis et purus mattis, blandit dictum tellus. \\n\\nMaecenas non lorem quis tellus placerat varius. Nulla facilisi. Aenean congue fringilla justo ut aliquam. Mauris id ex erat. Nunc vulputate neque vitae justo facilisis, non condimentum ante sagittis. Morbi viverra semper lorem nec molestie. Maecenas tincidunt est efficitur ligula euismod, sit amet ornare est vulputate.\\n\\nIn non mauris justo. Duis vehicula mi vel mi pretium, a viverra erat efficitur. Cras aliquam est ac eros varius, id iaculis dui auctor. Duis pretium neque ligula, et pulvinar mi placerat et. Nulla nec nunc sit amet nunc posuere vestibulum. Ut id neque eget tortor mattis tristique. Donec ante est, blandit sit amet tristique vel, lacinia pulvinar arcu. Pellentesque scelerisque fermentum erat, id posuere justo pulvinar ut. Cras id eros sed enim aliquam lobortis. Sed lobortis nisl ut eros efficitur tincidunt. Cras justo mi, porttitor quis mattis vel, ultricies ut purus. Ut facilisis et lacus eu cursus.\\n\\nIn eleifend velit vitae libero sollicitudin euismod. \\n\\nFusce vitae vestibulum velit. Pellentesque vulputate lectus quis pellentesque commodo. Aliquam erat volutpat. Vestibulum in egestas velit. Pellentesque fermentum nisl vitae fringilla venenatis. Etiam id mauris vitae orci maximus ultricies. Cras fringilla ipsum magna, in fringilla dui commodo a.\\n\\nEtiam vehicula luctus fermentum. In vel metus congue, pulvinar lectus vel, fermentum dui. Maecenas ante orci, egestas ut aliquet sit amet, sagittis a magna. Aliquam ante quam, pellentesque ut dignissim quis, laoreet eget est. Aliquam erat volutpat. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Ut ullamcorper justo sapien, in cursus libero viverra eget. Vivamus auctor imperdiet urna, at pulvinar leo posuere laoreet. Suspendisse neque nisl, fringilla at iaculis scelerisque, ornare vel dolor. Ut et pulvinar nunc. Pellentesque fringilla mollis efficitur. Nullam venenatis commodo imperdiet. Morbi velit neque, semper quis lorem quis, efficitur dignissim ipsum. Ut ac lorem sed turpis imperdiet eleifend sit amet id sapien.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet unstructured"
      ],
      "metadata": {
        "id": "P8a2UqhM0VOE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "loader = UnstructuredURLLoader('https://genai-weekly-show-case.hashnode.dev/overcoming-challenges-in-building-a-voice-to-voice-chatbot-a-journey-with-hugging-face-github-and-api-integration')"
      ],
      "metadata": {
        "id": "31hgsUszxCxY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader('https://genai-weekly-show-case.hashnode.dev/overcoming-challenges-in-building-a-voice-to-voice-chatbot-a-journey-with-hugging-face-github-and-api-integration')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gDfv0yt1Rez",
        "outputId": "e7a01141-0372-44d8-a7be-a79047c18074"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "rHQ0jV-W0y3k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tzPLKKz_08DU",
        "outputId": "4f11d027-b0a9-4171-e69c-e6219134d089"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://genai-weekly-show-case.hashnode.dev/overcoming-challenges-in-building-a-voice-to-voice-chatbot-a-journey-with-hugging-face-github-and-api-integration', 'title': 'Overcoming Challenges in Building a Voice-to-Voice Chatbot: A Journey with Hugging Face, GitHub, and API Integration', 'description': 'Learn how to build a real-time voice-to-voice chatbot using Whisper, Groq API, and gTTS, and deploy it on Hugging Face Spaces', 'language': 'en'}, page_content='Overcoming Challenges in Building a Voice-to-Voice Chatbot: A Journey with Hugging Face, GitHub, and API Integrationgenai-weekly-show-caseFollowgenai-weekly-show-caseFollowOvercoming Challenges in Building a Voice-to-Voice Chatbot: A Journey with Hugging Face, GitHub, and API IntegrationAbuBakar khan lakhwera·Jan 11, 2025·5 min readIn this article, I will share the journey of creating a Voice-to-Voice Chatbot using OpenAI\\'s Whisper, Groq API, and Google\\'s Text-to-Speech (gTTS). The project aimed to develop an interactive real-time chatbot that takes voice input, processes it, and responds with a voice output. Here\\'s a breakdown of the approach, challenges, and solutions involved in building the chatbot, and how I integrated it into a simple Gradio interface for deployment on Hugging Face Spaces.\\nThe Project Overview\\nThe goal was simple yet ambitious: Create a real-time voice interaction system that:\\n\\nUses Whisper to transcribe spoken input into text.\\n\\nSends the transcription to Groq\\'s API to interact with a language model (LLM).\\n\\nConverts the LLM’s response into speech using gTTS.\\n\\n\\nI used Google Colab for development and Gradio for easy interface creation and deployment on Hugging Face Spaces. Throughout this project, I faced some challenges, but with the help of Python libraries and thoughtful error handling, I was able to overcome them.\\nThe Tech Stack\\n\\nWhisper (OpenAI): Used for speech-to-text.\\n\\nGroq API: Served as the LLM backend, where I sent the transcribed text for a response.\\n\\ngTTS: Converted the text response from the LLM to speech.\\n\\nGradio: Used for creating a user-friendly interface to interact with the chatbot.\\n\\n\\nKey Components of the Chatbot\\n\\nSpeech-to-Text with Whisper: Whisper is a powerful model for converting speech to text, and it works directly with audio files in various formats. I used Whisper to transcribe the user’s voice input.\\n\\nQuerying the Groq LLM: Once the transcription was complete, the text was sent to Groq\\'s API to interact with the Llama model and generate a relevant response. The model I used is “llama-3.3-70b-versatile,” chosen for its ability to handle diverse conversational contexts.\\n\\nText-to-Speech with gTTS: The response from Groq was in text format, which I used gTTS to convert back to speech, creating a complete voice-to-voice interaction.\\n\\nGradio Interface: I leveraged Gradio to create a clean and intuitive interface where users can speak into their microphone, and the chatbot responds with audio. This made it easy to deploy and share the project.\\n\\n\\nChallenges Faced\\nChallenge #1: Handling the API Key Securely\\nOne of the first obstacles I faced was how to securely manage the Groq API key without exposing it in the code, especially when deploying on Hugging Face Spaces.\\nSolution:\\nI used environment variables to keep the API key secure. In Google Colab, I set it directly using:\\npythonCopy codeimport os\\nos.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\"\\n\\nOn Hugging Face Spaces, I stored the key as a Secret in the Space’s settings, ensuring it stayed hidden from the public.\\nChallenge #2: Audio Input Handling\\nThe second challenge was correctly handling the audio input from Gradio\\'s microphone source. Initially, I had trouble with the input format, leading to errors.\\nSolution:\\nI resolved this by carefully managing the audio data format. I used soundfile to read the audio in the correct format and transcribe it via Whisper. I ensured the audio file was saved in WAV format for proper compatibility with Whisper\\'s transcription model.\\nChallenge #3: Debugging Groq API Responses\\nOnce the transcription was done, the next step was sending the text to the Groq API. However, at first, I didn’t get any response from the API, leaving the interface unresponsive.\\nSolution:\\nI added error handling in the code to catch any issues with the Groq API request. This way, if the transcription was empty or the API call failed, I could display a meaningful error message.\\nHere’s an example of the error handling for querying Groq:\\npythonCopy codedef query_groq_llm(transcription):\\n    try:\\n        if not transcription.strip():\\n            return \"No transcription available to query the LLM.\"\\n\\n        chat_completion = client.chat.completions.create(\\n            messages=[{\"role\": \"user\", \"content\": transcription}],\\n            model=\"llama-3.3-70b-versatile\",\\n            stream=False,\\n        )\\n        return chat_completion.choices[0].message.content\\n    except Exception as e:\\n        return f\"Error querying the LLM: {e}\"\\n\\nChallenge #4: Voice Output Handling\\nOnce the response came back from the Groq API, the next step was to convert it back to audio using gTTS. The issue here was ensuring the audio played correctly after conversion.\\nSolution:\\nI used temporary audio files to store the generated speech and returned the file path to Gradio, which then played the response audio.\\nFinal Solution and Code\\nHere is the complete solution, which integrates Whisper, Groq, and gTTS seamlessly:\\npythonCopy codeimport os\\nimport tempfile\\nimport gradio as gr\\nfrom groq import Groq\\nfrom gtts import gTTS\\nimport whisper\\nimport soundfile as sf\\nimport io\\n\\n# Initialize Groq client\\nclient = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\\nwhisper_model = whisper.load_model(\"base\")\\n\\ndef transcribe_audio(audio):\\n    audio_data, samplerate = sf.read(io.BytesIO(audio))\\n    sf.write(\"input.wav\", audio_data, samplerate)\\n    result = whisper_model.transcribe(\"input.wav\")\\n    return result[\"text\"]\\n\\ndef query_groq_llm(transcription):\\n    chat_completion = client.chat.completions.create(\\n        messages=[{\"role\": \"user\", \"content\": transcription}],\\n        model=\"llama-3.3-70b-versatile\",\\n        stream=False,\\n    )\\n    return chat_completion.choices[0].message.content\\n\\ndef text_to_speech(response):\\n    tts = gTTS(text=response, lang=\"en\")\\n    temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\\n    tts.save(temp_audio.name)\\n    return temp_audio.name\\n\\ndef chatbot_pipeline(audio):\\n    transcription = transcribe_audio(audio)\\n    response = query_groq_llm(transcription)\\n    audio_response = text_to_speech(response)\\n    return transcription, response, audio_response\\n\\ndef interface(audio):\\n    transcription, response, audio_response = chatbot_pipeline(audio)\\n    return transcription, response, (audio_response,)\\n\\niface = gr.Interface(\\n    fn=interface,\\n    inputs=gr.Audio(type=\"numpy\", label=\"Speak into the microphone\"),\\n    outputs=[gr.Textbox(label=\"Transcription\"), gr.Textbox(label=\"LLM Response\"), gr.Audio(label=\"Response Audio\")],\\n    title=\"Real-Time Voice-to-Voice Chatbot\",\\n    description=\"Speak into the microphone, and the chatbot will respond with audio!\"\\n)\\n\\niface.launch(debug=True)\\n\\nDeployment\\nTo deploy the project on Hugging Face Spaces, I followed these steps:\\n\\nSave the code in a file named app.py.\\n\\nCreate a requirements.txt with the necessary dependencies:\\n txtCopy codegradio\\n gtts\\n whisper\\n openai-groq-api\\n soundfile\\n\\n\\nPush the code and files to a Hugging Face Space.\\n\\n\\nConclusion\\nBuilding the Voice-to-Voice Chatbot was a challenging yet rewarding process. I had to integrate several technologies, handle multiple APIs, and troubleshoot various issues. However, by carefully handling errors, securing API keys, and ensuring compatibility between the different components, I was able to create a chatbot that delivers a seamless voice interaction experience.\\nCheck out the Voice-to-Voice Chatbot on Hugging Face and the GitHub Repository for the source code.\\nI hope this article helps others who are looking to build similar applications. Let me know if you have any questions or feedback!\\n\\ngenerative ai\\xa0Share this')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[pdf]\"\n",
        "!pip install tesseract-ocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pp_yP3xY1XPE",
        "outputId": "2ee43dda-44af-4b44-bd0f-2da2b1af4588"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.11/dist-packages (0.16.23)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.13.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.12.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.30.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (20240706)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (9.5.2)\n",
            "Requirement already satisfied: pi-heif in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.21.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.3.0)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.10.0)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: unstructured-inference>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.8.7)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.3.13)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (0.28.1)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (4.11.0.86)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (1.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (2.5.1+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (1.0.14)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (4.48.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (1.13.1)\n",
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.7->unstructured[pdf]) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[pdf]) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[pdf]) (43.0.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (11.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[pdf]) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (0.20.1+cu124)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (5.29.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[pdf]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (2024.11.6)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pikepdf->unstructured[pdf]) (1.2.18)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured[pdf]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (2025.1.31)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (24.1.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.68.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (0.14.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[pdf]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[pdf]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[pdf]) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[pdf]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[pdf]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[pdf]) (3.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->unstructured-client->unstructured[pdf]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->unstructured-client->unstructured[pdf]) (2.27.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->unstructured-inference>=0.8.7->unstructured[pdf]) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.7->unstructured[pdf]) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[pdf]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference>=0.8.7->unstructured[pdf]) (0.21.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=0.8.7->unstructured[pdf]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=0.8.7->unstructured[pdf]) (2025.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[pdf]) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured-inference>=0.8.7->unstructured[pdf]) (3.0.2)\n",
            "Collecting tesseract-ocr\n",
            "  Downloading tesseract-ocr-0.0.1.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from tesseract-ocr) (3.0.12)\n",
            "Building wheels for collected packages: tesseract-ocr\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tesseract-ocr (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for tesseract-ocr\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for tesseract-ocr\n",
            "Failed to build tesseract-ocr\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tesseract-ocr)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "loader = UnstructuredPDFLoader('/content/Noisy-label_Learning_with_Sample_Selection_based_o.pdf')"
      ],
      "metadata": {
        "id": "dS0lGujd3TAT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents  = loader.load()"
      ],
      "metadata": {
        "id": "D8w1b-Dp4gKS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, doc in enumerate(documents):\n",
        "    print(f\"Page {idx + 1} Content:\\n\")\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lXb4SxrQ4lYR",
        "outputId": "163f11b6-c03e-458d-c8b6-ebe86a86447b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 Content:\n",
            "\n",
            "3 2 0 2\n",
            "\n",
            "y a M 1 3\n",
            "\n",
            "]\n",
            "\n",
            "V C . s c [\n",
            "\n",
            "1 v 6 8 4 9 1 . 5 0 3 2 : v i X r a\n",
            "\n",
            "Noisy-label Learning with Sample Selection based on Noise Rate Estimate\n",
            "\n",
            "Arpit Garg Australian Institute for Machine Learning University of Adelaide Australia arpit.garg@adelaide.edu.au\n",
            "\n",
            "Cuong Nguyen Australian Institute for Machine Learning University of Adelaide Australia cuong.nguyen@adelaide.edu.au\n",
            "\n",
            "Rafael Felix Australian Institute for Machine Learning University of Adelaide Australia rafael.felixalves@adelaide.edu.au\n",
            "\n",
            "Thanh-Toan Do Department of Data Science and AI Monash University Australia toan.do@monash.edu\n",
            "\n",
            "Gustavo Carneiro Centre for Vision, Speech and Signal Processing University of Surrey United Kingdom g.carneiro@surrey.ac.uk\n",
            "\n",
            "Abstract\n",
            "\n",
            "Noisy-labels are challenging for deep learning due to the high capacity of the deep models that can overfit noisy-label training samples. Arguably the most realistic and coincidentally challenging type of label noise is the instance-dependent noise (IDN), where the labelling errors are caused by the ambivalent information present in the images. The most successful label noise learning techniques to address IDN problems usually contain a noisy-label sample selection stage to separate clean and noisy-label samples during training. Such sample selection depends on a criterion, such as loss or gradient, and on a curriculum to define the proportion of training samples to be classified as clean at each training epoch. Even though the estimated noise rate from the training set appears to be a natural signal to be used in the definition of this curriculum, previous approaches generally rely on arbitrary thresholds or pre-defined selection functions to the best of our knowledge. This paper addresses this research gap by proposing a new noisy-label learning graphical model that can easily accommodate state-of-the-art (SOTA) noisy-label learning methods and provide them with a reliable noise rate estimate to be used in a new sample selection curriculum. We show empirically that our model integrated with many SOTA methods can improve their results in many IDN benchmarks, including synthetic and real-world datasets.\n",
            "\n",
            "Preprint. Under review.\n",
            "\n",
            "1\n",
            "\n",
            "Introduction\n",
            "\n",
            "Deep neural networks (DNNs) attain exceptional performance in countless tasks across many do- mains, including vision [41], language [36], medical [32], and code-generation [9]. Yet, such accomplishments usually depend on the methodical curation of training sets with clean-labels, which can be extraordinarily expensive in some domains [37]. Cost-effective labelling techniques are beneficial [44] in these cases (such as data mining [8] and crowd-sourcing [34]) but it often results in inferior-standard labelling [34]. Thus, such labelling can introduce incorrect labels in real-world datasets [17]. Even small amounts of label noise are enough to hinder the effectiveness of DNNs due to the well-known memorisation effects [47, 28]. This problem motivated the designing of robust noisy-label learning algorithms.\n",
            "\n",
            "The type of label noise, i.e., instance-independent noise (IIN) [13] or instance-dependent noise (IDN) [38], dictates the design principles of the noisy-label learning algorithms. For instance, IIN focuses on mislabellings that are independent of sample information [13], where estimating the underlying label transition matrix is a common way of handling this noise type [44]. On the other hand, in the more-realistic IDN, mislabellings are due to both sample information and true class labels [38], which generally require the combination of many label noise learning techniques, such as robust loss functions [50, 22], and noisy-label sample selection [20, 51]. In particular, sample selection approaches that divide the training data into clean and noisy samples have produced competitive results in many benchmarks [20, 6, 8, 17, 12]. Such sample selection techniques require the definition of a criterion and a selection curriculum. Many studies in this topic focus on developing new sample selection criteria, such as the small-loss hypothesis [20], which states that noisy-label samples have larger loss values than clean-label samples, particularly during the first training stages [1]. Another example is the FINE [17] criterion, which discriminates clean and noisy-label samples via the distance to class-specific eigenvectors. In this technique, clean-label samples tend to lie closer to the class- specific dominant eigenvector of the latent representations than the noisy-label samples. One more example is SSR [8], which introduces a selection criterion based on K nearest neighbour (KNN) classification in the feature space. Further, CC [51] uses a two-stage sampling procedure, including class-level feature clustering followed by a consistency score. An equally important problem in sample selection is the definition of the curriculum to select clean training samples, but it has received comparatively less attention.\n",
            "\n",
            "The sample selection curriculum defines a threshold to be used with one of the criteria listed above to classify the training samples into clean or noisy at each training epoch [39]. For example, the threshold can be fixed to an arbitrary clustering score that separates clean and noisy samples [20], but such strategy does not account for the proportion of label noise in the training set, nor does it consider the dynamics of the selection of noisy-label samples during the training. The consideration of such dynamics has been studied in [13, 43], which defined a curriculum of the noisy-label sampling rate R(t) as a function of the training epoch t ∈ {1,...,T}. The curriculum R(t) defines a sampling rate close to 100% of the training set at the beginning of the training, which is then reduced to arbitrarily low rates at the end of the training. In practice, the function R(t) is either pre-defined [13] or learned by weighting a set of basis functions with similar characteristics [43]. Although generally effective, these techniques do not consider the label noise rate estimated from the training set, making them vulnerable to over-fitting (if too many noisy-label samples are classified as clean) or under-fitting (if informative clean-label samples are classified as noisy). It can be argued that label transition matrix estimation [44, 5, 42] aims to recover the noise rate affecting pairwise label transitions. However, label transition matrix techniques follow a quite different strategy compared with sample selection methods, where their main challenge is the general under-constrained aspect of the matrix estimation, making them sensitive to large noise rates and not scalable to a high number of classes [34]. We are unaware of any approach that aims to directly estimate the label noise rate from the training set and incorporate this rate into the sample selection curriculum.\n",
            "\n",
            "To motivate the use of noise rate to select noisy-label samples during training, let us consider CIFAR100 [18] at an instance-dependent noise rate ϵ = 50% [38] (noise rate specifications and other details are explained in Section 4). We use DivideMix [20], but replace its sample selection (based on an arbitrary clustering score [7, 35, 26]) by a thresholding process that classifies the R(t) = 1 − ϵ = 50% largest loss samples as noisy, and the remaining ones as clean in all training epochs t ∈ {1,...,T}. This sample selection is used to implement the semi-supervised learning mechanism of DivideMix. As displayed in Fig. 1a, the new sample selection approach based on the\n",
            "\n",
            "2\n",
            "\n",
            "65\n",
            "\n",
            "Observable\n",
            "\n",
            "Hidden\n",
            "\n",
            "Noise rate\n",
            "\n",
            "ϵ\n",
            "\n",
            "% y c a r u c c A\n",
            "\n",
            "60\n",
            "\n",
            "DivideMix DivideMix with ϵ = 0.5\n",
            "\n",
            "X\n",
            "\n",
            "Y\n",
            "\n",
            "ˆY\n",
            "\n",
            "55\n",
            "\n",
            "N\n",
            "\n",
            "50\n",
            "\n",
            "100\n",
            "\n",
            "150 200 № of epochs\n",
            "\n",
            "250\n",
            "\n",
            "300\n",
            "\n",
            "ρ\n",
            "\n",
            "θy\n",
            "\n",
            "θˆy\n",
            "\n",
            "(a) Motivation\n",
            "\n",
            "(b) Proposed Graphical Model\n",
            "\n",
            "Figure 1: (a) Comparison of test accuracy % (as a function of training epoch) between the original DivideMix [20] (solid, blue curve) and our modified DivideMix (dashed, red curve) that selects the clean and noisy data based on a fixed noise rate R(t) = 1 − ϵ = 50% using the small-loss criterion on CIFAR100 [18] at 0.5 IDN [38]; (b) The proposed probabilistic graphical model that generates noisy-label ˆY conditioned on the image X, the latent clean-label Y and noise rate ϵ, where forward pass (solid lines) is parameterized by θy,θˆy and ϵ representing the generation step, and the backward pass (dashed lines) is parameterized by ρ.\n",
            "\n",
            "“provided” noise rate (dashed red curve) improves 6% in terms of prediction accuracy as compared with the original DivideMix [20] (solid blue curve) that relies on arbitrary thresholding. Similar conclusions can be achieved with other methods that apply sample selection strategies to address the noisy-label learning problem, as shown in the experiments.\n",
            "\n",
            "In this paper, we introduce a new noisy-label learning graphical model (shown in Fig. 1b) that can be integrated with SOTA noisy-label learning methods to provide them with a reliable noise rate estimate and a new sample selection curriculum. In particular, in our curriculum, instead of being constrained by a pre-defined function R(t) [13, 43], it is based on a noise rate automatically estimated from the training set, as displayed in Fig. 2a. The integration of our graphical model with SOTA noisy-label learning models (e.g., DivideMix [20], C2D [52], InstanceGM [12], FINE [17], SSR [8], and CC [51]) is shown to improve their sample selection mechanisms, and ultimately upgrade their results, as presented in Fig. 2b. The primary contributions of our paper can be summarised as follows:\n",
            "\n",
            "A novel noisy-label learning graphical model (see Fig. 1b) that estimates and uses the noise rate from the training set to build a new sample selection curriculum.\n",
            "\n",
            "A simple strategy to integrate our new graphical model with many SOTA noisy-label learning methods, such as DivideMix [20], C2D [52], InstanceGM [12], FINE [17], SSR [8], and CC [51] with the goal of improving their sample selection process, and consequently their test accuracy, as displayed in Fig. 2b.\n",
            "\n",
            "We also demonstrate empirically the critical role of the new sample selection mechanism that boosts the performance of SOTA noisy-label learning methods on several synthetic (CIFAR100 [18]) and real-world (red mini-ImageNet [15], Clothing1M [39], mini-WebVision [20] and ImageNet [19]) benchmarks.\n",
            "\n",
            "2 Method\n",
            "\n",
            "In this section, we present our new graphical model that estimates the noise rate, which will be used in the sample selection process. Let D = {(xi, ˆyi)}N i=1 be the noisy-label training set containing d-dimensional data vector xi ∈ X ⊆ Rd and it’s respective C-dimensional one-hot encoded observed (potentially corrupted) label ˆyi ∈ ˆY = {ˆy : ˆy ∈ {0,1}C ∧ 1⊤ Cˆy = 1}, where 1C is a vector of ones with C dimensions. The aim is to estimate the label noise rate ϵ, used for the generation of noisy-label training data from the observed training dataset D and integrate this label noise rate into the sample selection strategy.\n",
            "\n",
            "3\n",
            "\n",
            "1.2\n",
            "\n",
            "1\n",
            "\n",
            "Co-teaching Tk = 5 Co-teaching Tk = 10 S2E\n",
            "\n",
            ")\n",
            "\n",
            "%\n",
            "\n",
            "80\n",
            "\n",
            "Base\n",
            "\n",
            "Ours\n",
            "\n",
            "9 1 . 7 7\n",
            "\n",
            "6 7 . 7 7\n",
            "\n",
            "(\n",
            "\n",
            ") t ( R\n",
            "\n",
            "0.8\n",
            "\n",
            "0.6\n",
            "\n",
            "0.4\n",
            "\n",
            "Ours\n",
            "\n",
            "y c a r u c c A\n",
            "\n",
            "70\n",
            "\n",
            "60\n",
            "\n",
            "1 6 . 8 5\n",
            "\n",
            "2 0 . 4 6\n",
            "\n",
            "9 8 . 9 5\n",
            "\n",
            "2 0 . 0 6\n",
            "\n",
            "0\n",
            "\n",
            "100\n",
            "\n",
            "200\n",
            "\n",
            "300\n",
            "\n",
            "DivideMix\n",
            "\n",
            "F-DivideMix\n",
            "\n",
            "InstanceGM\n",
            "\n",
            "№ of epochs T\n",
            "\n",
            "Models\n",
            "\n",
            "(a) Curriculum function R(t)\n",
            "\n",
            "(b) Accuracy Comparison\n",
            "\n",
            "Figure 2: (a) Visual comparison of different R(t) on CIFAR100 [18] at 0.5 IDN [38]: (i) Co- teaching [13] with curricula based on different hyper-parameter Tk, where R(t) = 1−τ ·min(t/Tk,1) and we manually set τ = ϵ = 0.5; (ii) S2E [43], where R(t) is estimated with a bi-level optimisation; and (iii) ours. (b) Comparison between noisy-label robust methods on CIFAR100 [18] at 0.5 IDN [38], including DivideMix [20], FINE [17] and InstanceGM [12], without (left, blue) and with (right, orange) integration of our proposed graphical model for estimation of the noise rate ϵ and sample selection based on ϵ.\n",
            "\n",
            "Expectation\n",
            "\n",
            "ˆY = Dog\n",
            "\n",
            "Clean\n",
            "\n",
            "X, ˆY\n",
            "\n",
            "q(Y |X, ˆY ; ρ)\n",
            "\n",
            "Y\n",
            "\n",
            "ρ\n",
            "\n",
            "Noisy-label robust model\n",
            "\n",
            "Noisy\n",
            "\n",
            "Maximisation\n",
            "\n",
            "Noisy-label robust classifier\n",
            "\n",
            "ϵ\n",
            "\n",
            "Sample Selection\n",
            "\n",
            "X\n",
            "\n",
            "P(Y |X; θy)\n",
            "\n",
            "Y\n",
            "\n",
            "P( ˆY |X, Y ; θˆy, ϵ)\n",
            "\n",
            "ˆY\n",
            "\n",
            "D = {(X, ˆY )}\n",
            "\n",
            "Clean\n",
            "\n",
            "Noisy\n",
            "\n",
            "θy\n",
            "\n",
            "X\n",
            "\n",
            "θˆy\n",
            "\n",
            "CE Loss\n",
            "\n",
            "X, ˆY\n",
            "\n",
            "Training\n",
            "\n",
            "ˆY\n",
            "\n",
            "Figure 3: Our training algorithm uses an existing noisy-label classifier (e.g., DivideMix [20]) parameterised by θy as the clean-label model p(Y |X;θy). The generation of noisy-label (given X and Y ) is performed by a model parameterised by noise rate ϵ and θˆy. The noisy-label classifier relies on a sample-selection mechanism that uses a curriculum R(t) = 1 − ϵ(t).\n",
            "\n",
            "2.1 Graphical Model\n",
            "\n",
            "We portray the generation of noisy-label via the probabilistic graphical model shown in Fig. 1b. The observed random variables, denoted by shaded circles, are data X and the corresponding noisy-label ˆY . We also have one latent variable, namely: the clean-label Y . Under our proposed modelling assumption, a noisy-label of a data instance can be generated as follows:\n",
            "\n",
            "sample an instance from the pool of data p(X), i.e.,: x ∼ p(X) • sample a clean-label from the clean-label distribution: y ∼ Cat(Y ;fθy(x)) • sample a noisy-label from the noisy-label distribution: ˆy ∼ Cat(ˆY ;ϵ×fθˆy(x)+(1−ϵ)×y),\n",
            "\n",
            "where Cat(.) denotes a categorical distribution, fθy : X → ∆C−1 and fθˆy : X × ∆C−1 → ∆C−1 denote two classifiers for the clean-label Y and noisy-label ˆY , respectively, with ∆C−1 = {s : s ∈ [0,1]C ∧111Cs = 1} being the (C − 1)-dimensional probability simplex.\n",
            "\n",
            "4\n",
            "\n",
            "According to the data generation process, ϵ corresponds to E (x,ˆy)∼p(X,ˆY )[P(ˆy ̸= y|x)], which is the label noise rate of the training dataset of interest. Our aim is to infer the parameters θy,θˆy and ϵ from a noisy-label dataset D by maximising the following log-likelihood:\n",
            "\n",
            "max θy,θˆy,ϵ\n",
            "\n",
            "E(xi,ˆyi)∼D [lnp(ˆyi|xi;θy,θˆy,ϵ)] = max θy,θˆy,ϵ\n",
            "\n",
            "E(xi,ˆyi)∼D\n",
            "\n",
            "(cid:104) ln(cid:80)\n",
            "\n",
            "yi\n",
            "\n",
            "(cid:105) p(ˆyi,yi|xi;θy,θˆy,ϵ)\n",
            "\n",
            "Due to the presence of the clean-label yi, it is difficult to evaluate the log-likelihood in Eq. (1) directly. We, therefore, employ the expectation - maximisation (EM) algorithm [7] to maximise the log-likelihood. The main idea of the EM algorithm is to (i) construct a tight lower bound of the likelihood in Eq. (1) by estimating the latent variable Y (known as expectation step) and (ii) maximise that lower bound (known as maximisation step). Formally, let q(yi|x, ˆy;ρ) be an arbitrary distribution over a clean-label yi. The evidence lower bound (ELBO) on the log-likelihood in Eq. (1) can be obtained through Jensen’s inequality and presented as follows:\n",
            "\n",
            "Q(θy,θˆy,ϵ,ρ) = E(xi,ˆyi)∼D [lnp(ˆyi|xi;θy,θˆy,ϵ) − KL[q(yi|xi, ˆyi;ρ)∥p(yi|xi, ˆyi)]]\n",
            "\n",
            "= E(xi,ˆyi)∼D\n",
            "\n",
            "(cid:2)Eq(yi|xi,ˆyi;ρ)[lnp(yi|xi;θy) + lnp(ˆyi|xi,yi;θˆy,ϵ))] + H[q(yi|xi, ˆyi;ρ)](cid:3),\n",
            "\n",
            "where KL[q∥p] is the Kullback – Leibler divergence between distributions q and p, and H(q) is the entropy of the distribution q.\n",
            "\n",
            "The EM algorithm is then carried out iteratively by alternating the following two steps:\n",
            "\n",
            "E step We maximise the ELBO in Eq. (2) w.r.t. q(yi|xi, ˆyi;ρ). Theoretically, such optimisation results in KL[q(yi|xi, ˆyi;ρ)∥p(yi|xi, ˆyi)] = 0 or q(yi|xi, ˆyi;ρ) = p(yi|xi, ˆyi). This is equivalent to estimating the posterior of the clean-label yi given noisy-label data (xi, ˆyi). Obtaining the exact posterior p(yi|xi, ˆyi) is, however, intractable for most deep-learning models. To mitigate such an issue, we follow the variational EM approach [27] by employing an approximate posterior q(yi|xi, ˆyi;ρ(t)) that is the closest to the true posterior p(yi|xi, ˆyi), where:\n",
            "\n",
            "ρ(t) = argmaxρ Q(θ(t)\n",
            "\n",
            "y ,θ(t)\n",
            "\n",
            "ˆy ,ϵ(t),ρ),\n",
            "\n",
            "with the superscript (t) denoting the parameters at the t-th iteration. Although this results in a non-tight lower bound of the log-likelihood in Eq. (1), it does increase the variational bound Q.\n",
            "\n",
            "M step We maximise the ELBO in Eq. (2) w.r.t. θy,θˆy and ϵ:\n",
            "\n",
            "θ(t+1) y\n",
            "\n",
            ",θ(t+1) ˆy\n",
            "\n",
            ",ϵ(t+1) = argmaxθy,θˆy,ϵ Q(cid:0)θy,θˆy,ϵ,ρ(t)(cid:1).\n",
            "\n",
            "The estimated noise rate ϵ can then be integrated into certain noisy-label algorithms to train the models of interest as mentioned in Section 1. Despite its effectiveness shown in Fig. 1a, such a two-phase process might be inefficient. In addition, the inference of noise rate ϵ might associate with the identifiability issue when estimating the clean-label Y [23], i.e., there exists multiple sets of ρ and θy, where each set can explain the observed noisy-label data equally well. Such issues are addressed in the following subsection.\n",
            "\n",
            "2.2 Sample Selection for SOTA Models to Address the Identifiability Issue\n",
            "\n",
            "The identifiability issue when inferring the clean-label Y from noisy-label data (X, ˆY ) can be mitigated either by acquiring multiple noisy-labels [23] or introducing additional constraints, such as small loss hypothesis [13] or FINE [17]. Since requesting additional noisy-labels per training sample is not always available, we follow the latter approach by imposing a constraint, denoted as L(θy,ϵ(t)), over θy in the M step via a sample selection approach based on the estimated noise rate ϵ(t). Formally, we propose a new curriculum when selecting samples as follows:\n",
            "\n",
            "R(t) = 1 − ϵ(t).\n",
            "\n",
            "In the simplest case, such as Co-teaching [13] or FINE [17], the constraint for θy can be written as:\n",
            "\n",
            "L(θy,ϵ(t)) = (cid:80)\n",
            "\n",
            "(xi,ˆyi)∈Sclean\n",
            "\n",
            "KL(cid:2)Cat(Y ; ˆy)∥Cat(Y ;fθy(xi))(cid:3),\n",
            "\n",
            "5\n",
            "\n",
            ".\n",
            "\n",
            "(1)\n",
            "\n",
            "(2)\n",
            "\n",
            "(3)\n",
            "\n",
            "(4)\n",
            "\n",
            "(5)\n",
            "\n",
            "(6)\n",
            "\n",
            "Algorithm 1 Proposed noisy-label learning algorithm that relies on the estimation of noise rate ϵ to build a sample selection curriculum.\n",
            "\n",
            "1: procedure NOISE RATE ESTIMATION AND INTEGRATION(D,T,λ) 2: 3: 4: 5:\n",
            "\n",
            "▷ D = {(xi, ˆyi)}N ▷ T: number of epochs ▷ λ: a hyper-parameter Initialise θ(1) ˆy ,ϵ(1) and ρ(0) θ1 y ← WARM UP(D,θ1 y) t ← 0 for nepoch = 1 : T do\n",
            "\n",
            "◁ ◁ ◁\n",
            "\n",
            "i=1: training set with noisy-label data\n",
            "\n",
            "y ,θ(1)\n",
            "\n",
            "6: 7: 8: 9: 10: 11:\n",
            "\n",
            "for each mini-batch S in shuffle(D) do\n",
            "\n",
            "t ← t + 1 Sclean,Snoisy ← SAMPLE SELECTION(S,θ(t) ρ(t) ← VARIATIONAL E STEP(S,θ(t) θ(t+1) y return θy\n",
            "\n",
            "y ,ϵ(t)) ˆy ,ϵ(t),ρ(t−1)) y ,θ(t)\n",
            "\n",
            "▷ Eq. (7) ▷ Eq. (3)\n",
            "\n",
            "y ,θ(t)\n",
            "\n",
            "12:\n",
            "\n",
            ",θ(t+1) ˆy\n",
            "\n",
            ",ϵ(t+1) ← M STEP(Sclean,Snoisy,θ(t)\n",
            "\n",
            "ˆy ,ϵ(t),ρ(t),λ)\n",
            "\n",
            "▷ Eq. (8) ▷ parameter of the clean-label classifier\n",
            "\n",
            "13: 14:\n",
            "\n",
            "where:\n",
            "\n",
            "Zsorted = sort(z1,z2,...,zN) Sclean = {(xi, ˆyi) : (xi, ˆyi) ∈ D ∧ zi ∈ Zsorted ∧ i ≤ ⌊R(t) × N⌋}, Snoisy = D \\ Sclean,\n",
            "\n",
            "(7)\n",
            "\n",
            "with R(t) defined in (5), sort(.) representing a function that sorts the set of criterion (loss [20, 52, 51], distance to the largest eigenvectors [17], or KNN scores [8]) values {z1,z2,...,zN} in ascending order and ⌊.⌋ denoting the floor function.\n",
            "\n",
            "Intuitively, the loss in Eq. (6) is simply the cross-entropy loss on the ⌊R(t) × N⌋ clean samples selected based on the estimated noise rate ϵ(t). One can also extend to other SOTA models by replacing the loss L accordingly. For example, if DivideMix is used as a base model to constrain θy, L will include two addition terms: loss on un-labelled data and regularisation using mixup [48].\n",
            "\n",
            "2.3 Training and Testing\n",
            "\n",
            "Given the sample selection approach in Section 2.2, the training of the model is slightly modified to include the loss for the SOTA model from Eq. (6). The M step in Eq. (4) is, therefore, re-defined as:\n",
            "\n",
            "θ(t+1) y\n",
            "\n",
            ",θ(t+1) ˆy\n",
            "\n",
            ",ϵ(t+1) = argmaxθy,θˆy,ϵ Q(cid:0)θy,θˆy,ϵ,ρ(t)(cid:1) − λL(θy,ϵ(t)),\n",
            "\n",
            "(8)\n",
            "\n",
            "where λ is a hyper-parameter and L is defined similar to Eq. (6).\n",
            "\n",
            "The training procedure is summarised in Algorithm 1 and visualised in Fig. 3. In the implementation, we integrate the proposed method into existing models, such as DivideMix [20] or FINE [17]. Note that the clean-label classifier fθy(.) is also the clean classifier of the base model.\n",
            "\n",
            "3 Related Work\n",
            "\n",
            "DNNs suffer from overfitting when trained with noisy-labels [46], resulting in poor generalisation [22, 49]. To address this issue, several techniques have been developed, including noise-robust loss functions [24], noise-label sample selection [20, 17, 51], and re-labelling [8] followed by sample selection [8]. The most challenging and realistic type of label noise is IDN [34], where most methods have a training stage based on sample selection techniques [20, 52]. These sample selection techniques separate clean and noisy-label samples, where clean samples are treated as labelled, and noisy samples are discarded [13, 14, 43] or treated as unlabelled samples [20, 52, 29, 12, 6] for semi- supervised learning [2]. A major limitation of these approaches is the need to define a curriculum on how to select clean and noisy-label samples during training, which as discussed in Section 1, is either pre-determined [14, 13] or learned from a set of pre-determined basis functions [43]. Additionally, Xiao et al. [39] discuss the effectiveness of estimating the noise type, but no further investigation has\n",
            "\n",
            "6\n",
            "\n",
            "been devoted to estimating the noise rate. In general, we notice a research gap regarding the use of noise rate estimation to be used by sample selection techniques.\n",
            "\n",
            "The estimation of noise rate affecting the transition between pairs of labels has received considerable attention [38, 5, 53, 4]. Still, they portray comparatively lower accuracy results for large real-world datasets or for IDN problems [38, 34]. These label transition approaches suffer from identifiability issues [11], where any clean-label distribution assignment is acceptable as long as the distribution of observed labels can be reconstructed [10]. This makes the identification of the true underlying clean-labels challenging. One solution is to contemplate the use of multiple annotations to help analyse the agreements and disagreements for improved identification of clean patterns [25, 11].\n",
            "\n",
            "An alternate technique to handle IDN problems is based on graphical models representing the relationship between various observed and latent variables [12, 45, 39]. Garg et al. [12], Yao et al. [45] use graphical models which rely on a generative approach to generate noisy-labels from the respective image features and latent clean-labels. Nonetheless, previous graphical models fail to consider the underlying noise rate parameter while modelling. Our work is the first graphical model approach to estimate the noise rate of the dataset. An important point of our approach is that it can be easily integrated with existing SOTA noisy-label learning methods to improve classification accuracy.\n",
            "\n",
            "4 Experiments\n",
            "\n",
            "We show extensive experiments in several noisy-label synthetic benchmarks with CIFAR100 [18], and real-world benchmarks, including CNWL’s red mini-ImageNet [15], Clothing1M [39] and mini- WebVision [21]. Section 4.1 describes implementation details. We evaluate our approach by plugging SOTA models into p(y|x;θy), defined in Section 2, with results being shown in Section 4.2, and ablation studies in Section 4.3. Please refer to Appendix A.1 for detailed dataset information.\n",
            "\n",
            "4.1\n",
            "\n",
            "Implementation\n",
            "\n",
            "All methods are implemented in Pytorch [30] and use one NVIDIA RTX 3090 card. As mentioned in the original papers, hyperparameter settings are kept the same for the baselines used in the proposed algorithm. All classifier architectures are also kept the same as the baseline models. A random initialisation of noise rate parameter ϵ with the sigmoid as its activation function is employed for all experiments to maintain the fairness of the comparisons with other approaches. The value of λ in Eq. (8) is set to 1 for all the cases. We integrate many SOTA approaches [20, 52, 12, 17, 8] into our graphical model, as explained in Section 2.3. For CIFAR100 [18] with IDN [38], we integrate DivideMix [20] and InstanceGM [12] into our model, given their superior performance across various noise rates. Additionally, we also use F-Dividemix (Fig. 2b) from FINE [17]. Moreover, for red mini-ImageNet [15], we test our proposed approach with and without DINO self-supervision [3]. For the implementation without self-supervision, we use DivideMix [20] and InstanceGM [12], and for the self-supervised version, we only use InstanceGM [12]. The models for Clothing1M [39] are trained using DivideMix [20] and SSR [8]. Furthermore, for mini-WebVision [20] and ImageNet [19], we test our model with C2D [52]. Additional implementation details are present in Appendix A.2.\n",
            "\n",
            "4.2 Comparison\n",
            "\n",
            "This section compares our approach to the dataset with the IDN settings in Section 4.2.1 and noisy real-world settings in Section 4.2.2.\n",
            "\n",
            "4.2.1 Synthetic Instance-Dependent Noise\n",
            "\n",
            "The comparison between various baselines and our proposed work on CIFAR100 [18] with IDN [38] is shown in Table 1 with the noise rate ranging from 20% to 50%. It is worth noting that using our proposed model with DivideMix [20] and InstanceGM [12] improves their performance in 90% cases. Table 1 also shows the final noise rate ϵ estimated by our model, where the actual noise rates are displayed in the table’s header. It’s worth noting that the estimated noise rate is always reasonable to the actual rate.\n",
            "\n",
            "7\n",
            "\n",
            "Table 1: (left) Test accuracy % and (right) final estimated noise rate ϵ on CIFAR100 [18] under different IDN [39]. Other models’ results are from [12, 5]. Here, we integrate DivideMix [20] and InstanceGM [12] into our proposed model.\n",
            "\n",
            "Method\n",
            "\n",
            "Noise Rates - IDN\n",
            "\n",
            "CE [45] PartT [38] kMEIDTM [5]\n",
            "\n",
            "0.2\n",
            "\n",
            "30.42 65.33 69.16\n",
            "\n",
            "0.3\n",
            "\n",
            "24.15 64.56 66.76\n",
            "\n",
            "0.4\n",
            "\n",
            "21.45 59.73 63.46\n",
            "\n",
            "0.5\n",
            "\n",
            "14.42 56.80 59.18\n",
            "\n",
            "Estimated noise rates\n",
            "\n",
            "Method\n",
            "\n",
            "Actual noise rate\n",
            "\n",
            "0.2\n",
            "\n",
            "0.3\n",
            "\n",
            "0.4\n",
            "\n",
            "0.5\n",
            "\n",
            "DivideMix [20] DivideMix-Ours\n",
            "\n",
            "77.03 77.42\n",
            "\n",
            "76.33 58.61 70.80 77.21 72.41 64.02\n",
            "\n",
            "DivideMix 0.18 InstanceGM 0.33\n",
            "\n",
            "0.34 0.37\n",
            "\n",
            "0.47 0.42\n",
            "\n",
            "0.53 0.47\n",
            "\n",
            "79.69 InstanceGM [12] InstanceGM-Ours 79.61\n",
            "\n",
            "79.21 78.47 77.19 79.40 79.52 77.76\n",
            "\n",
            "Table 2: (left) Test accuracy % and (right) final estimated noise rate ϵ for red mini-ImageNet [15]. Other methods’ results are reported in [12, 40]. We present the results with and without self- supervision [3]. We integrate DivideMix [20] and InstanceGM [12] into our model, with the latter tested with and without self-supervision.\n",
            "\n",
            "Method\n",
            "\n",
            "Noise rate\n",
            "\n",
            "0.4\n",
            "\n",
            "0.6\n",
            "\n",
            "0.8\n",
            "\n",
            "CE [40] MixUp [48] MentorMix [15] FaMUS [40]\n",
            "\n",
            "42.70 46.40 47.14 51.42\n",
            "\n",
            "37.30 40.58 43.80 45.10\n",
            "\n",
            "29.76 33.58 33.46 35.50\n",
            "\n",
            "Estimated noise rates\n",
            "\n",
            "DivideMix [20] DivideMix-Ours\n",
            "\n",
            "46.72 43.14 50.70 45.11\n",
            "\n",
            "34.50 37.44\n",
            "\n",
            "Method\n",
            "\n",
            "Actual noise rate\n",
            "\n",
            "0.4\n",
            "\n",
            "0.6\n",
            "\n",
            "0.8\n",
            "\n",
            "InstanceGM [12] InstanceGM-Ours\n",
            "\n",
            "52.24 47.96 56.61 51.40\n",
            "\n",
            "39.62 43.83\n",
            "\n",
            "DivideMix InstanceGM InstanceGM-SS\n",
            "\n",
            "0.39 0.38 0.48\n",
            "\n",
            "0.58 0.55 0.53\n",
            "\n",
            "0.73 0.74 0.69\n",
            "\n",
            "With self-supervised learning\n",
            "\n",
            "PropMix [6]\n",
            "\n",
            "56.22\n",
            "\n",
            "52.84\n",
            "\n",
            "43.42\n",
            "\n",
            "56.37 53.21 InstanceGM-SS [12] InstanceGM-SS-Ours 58.29 53.60\n",
            "\n",
            "44.03 45.47\n",
            "\n",
            "4.2.2 Real-World Noise\n",
            "\n",
            "We have also evaluated our proposed method on various real-world noisy settings regarding test accuracy and estimated noise rates ϵ in Tables 2 to 4. Similarly to the synthetic IDN in Section 4.2.1, the results show that existing noisy-label robust methods can be easily integrated into our model to outperform current SOTA results for real-world noisy-label datasets. Table 2 shows the results on red mini-ImageNet using two configurations, including cases without self-supervision (top part of the table) and with self-supervision (bottom part of the table). The self-supervision DINO pre- training [3] relies only on images from red mini-ImageNet to enable a fair comparison with existing baselines [6, 12] Results from Table 2 demonstrate that our approach improves the performance In fact, using estimated noise rate ϵ of SOTA methods by a considerable margin in all cases. while training InstanceGM [12] without self-supervision shows better performance than existing self-supervised baselines at 0.4 and 0.8 noise rates. Moreover, DivideMix [20], SSR [8] and CC [51] are used as a baselines for Clothing1M [39] as shown in Table 3 and further explanation is present in ?? and ??. Furthermore, C2D [52] is used as a baseline for mini-WebVision [20], shown in Table 4 where validation is performed on ImageNet [19]. It is worth noting that results improve the most baselines and exhibit competitive performance.\n",
            "\n",
            "8\n",
            "\n",
            "Table 3: Test accuracy (%) of competing methods, and final estimated noise rate ϵ on Clothing1M [39]. Also, we have not considered competing models that rely on a clean set whilst training. We integrate DivideMix [20], SSR [8] and CC [51] into our model. DivideMix [20] and CC [51] shows the locally reproduced results. Estimated noise rate by [39] is 0.385. Our results are within 1% of the accuracy of the top approaches. Method\n",
            "\n",
            "Test accuracy (%) Estimated noise rate\n",
            "\n",
            "ELR+ with C2D [52] AugDesc [29] DivideMix [20] DivideMix-Ours SSR (class-imbalance) [8] SSR-Ours CC [8] CC-Ours\n",
            "\n",
            "74.58 75.11 74.32 74.41 74.12 74.20 75.24 75.31\n",
            "\n",
            "0.41\n",
            "\n",
            "0.42\n",
            "\n",
            "0.41\n",
            "\n",
            "Table 4: Test accuracy (%) and final estimated noise rate ϵ on mini-WebVision [20] and validation on ImageNet [19]. We integrate Contrast-to-Divide (C2D) [52] into our model, whilst C2D-Ours is our proposed approach. ImageNet [19] is only considered for validation.\n",
            "\n",
            "Dataset\n",
            "\n",
            "mini-WebVision\n",
            "\n",
            "ImageNet\n",
            "\n",
            "Estimated noise rate\n",
            "\n",
            "Top-1\n",
            "\n",
            "Top-5\n",
            "\n",
            "Top-1 Top-5\n",
            "\n",
            "DivideMix [20] BtR [33] SSR [8]\n",
            "\n",
            "77.32 80.88 80.92\n",
            "\n",
            "91.64 92.76 92.80\n",
            "\n",
            "75.20 75.96 75.76\n",
            "\n",
            "91.64 92.20 91.76\n",
            "\n",
            "C2D [52] C2D-Ours\n",
            "\n",
            "79.42 80.20\n",
            "\n",
            "92.32 92.82\n",
            "\n",
            "78.57 79.16\n",
            "\n",
            "93.04 93.12\n",
            "\n",
            "0.43\n",
            "\n",
            "4.3 Ablation\n",
            "\n",
            "We show an ablation study (left) and training time (right) of our approach in Table 5 on CIFAR100 [18] at 0.5 IDN [38] using DivideMix [20] as baseline. Initially, the accuracy result of baseline DivideMix under original settings is 58.61%. In the second row, we fix the noise rate ϵ at 0.5 for DivideMix’s sample selection, as explained in Section 2.3 (without updating ϵ), then the results improved to 64.44%, which is the ideal case that motivated our work (this is an ideal case because that would be a perfect noise rate estimation). In the third case, we use the proposed graphical model with pre-trained DivideMix [20] that shows an accuracy of 52.31%. In the next case, the proposed graphical model is trained together with DivideMix [20] without considering the estimated noise rate ϵ for sample selection, which results in an accuracy of 56.30%. In the last row, we show the training of the proposed model with DivideMix, together with the estimation of noise rate ϵ, and the selection of samples based on that, with ≈ 8% accuracy improvement, which is very close to our ideal case (second row).\n",
            "\n",
            "5 Conclusion\n",
            "\n",
            "In this paper, we demonstrate the importance of estimating the label noise rate to build a novel noisy-label sample selection curriculum. This estimation is a process within the training of our proposed graphical model that can effortlessly be integrated with SOTA noisy-label robust learning approaches to boost their classification accuracy on synthetic and real-world benchmarks that include CIFAR100 [18], red mini-ImageNet [15], Clothing1M [39], mini-WebVision [20] and ImageNet [19].\n",
            "\n",
            "This paper will encourage researchers and practitioners in the field to explore the noisy-label noise rate estimation for sample selection approaches. We aim to explore other (and perhaps more effective) ways to estimate noise rates. We also plan to investigate whether the estimated noise rate could be leveraged for other purposes beyond sample selection, such as for the study of theoretical aspects of noisy-label learning. We have not detected any negative societal impact but we anticipate many\n",
            "\n",
            "9\n",
            "\n",
            "Table 5: (left) Ablation results showing the test accuracy % on CIFAR100 [18] at 0.5 IDN [38]. First, we display the results of base DivideMix [20], followed by the ideal case (result in italics), where we assume to know the correct ϵ = 50% for the sample selection before training. In the third case, we incorporate a pre-trained DivideMix [20] into our model, whilst the next case shows our proposed model and DivideMix [20] being trained together, but using DivideMix’s [20] sample selection curriculum, not our estimated ϵ. The last row shows the results of our proposed complete model with DivideMix [20]. (right) This table shows the training time of the base model DivideMix [20] and our approach with DivideMix [20] at 0.5 IDN [38] on the mentioned computational settings Section 4.1. Models\n",
            "\n",
            "Test Accuracy (%)\n",
            "\n",
            "DivideMix DivideMix with fix ϵ (ideal case) Proposed with pre-trained DivideMix Proposed with original DivideMix (no ϵ) DivideMix-Ours\n",
            "\n",
            "58.61 64.44 52.31 56.30 64.02\n",
            "\n",
            "Model\n",
            "\n",
            "DivideMix [20] DivideMix-Ours\n",
            "\n",
            "Time (hrs)\n",
            "\n",
            "7.2 13.1\n",
            "\n",
            "positive societal impacts provided by estimating the noise rate and alleviating biases in the noisy data. For example, it can help to reduce the arduous annotation task for maintaining the data quality. We hope that this work would help other researchers to study the importance of estimating and using label noise rate in the designing of new noisy-label robust approaches.\n",
            "\n",
            "References\n",
            "\n",
            "[1] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In International Conference on Machine Learning, volume 70, pages 233–242. PMLR, 2017.\n",
            "\n",
            "[2] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, volume 32, 2019.\n",
            "\n",
            "[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand In International Conference on\n",
            "\n",
            "Joulin. Emerging properties in self-supervised vision transformers. Computer Vision, pages 9650–9660, 2021.\n",
            "\n",
            "[4] Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, and Pheng-Ann Heng. Beyond class-conditional assumption: A primary attempt to combat instance-dependent label noise. In AAAI Conference on Artificial Intelligence, pages 11442–11450, 2021.\n",
            "\n",
            "[5] De Cheng, Tongliang Liu, Yixiong Ning, Nannan Wang, Bo Han, Gang Niu, Xinbo Gao, and Masashi Sugiyama. Instance-dependent label-noise learning with manifold-regularized transition matrix estimation. In Conference on Computer Vision and Pattern Recognition, pages 16630–16639, 2022.\n",
            "\n",
            "[6] Filipe R Cordeiro, Vasileios Belagiannis, Ian Reid, and Gustavo Carneiro. PropMix: Hard sample filtering and proportional mixup for learning with noisy labels. In British Machine Vision Conference, 2021.\n",
            "\n",
            "[7] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1–22, 1977.\n",
            "\n",
            "[8] Chen Feng, Georgios Tzimiropoulos, and Ioannis Patras. SSR: An efficient and robust framework for\n",
            "\n",
            "learning with unknown label noise. In British Machine Vision Conference, 2022.\n",
            "\n",
            "[9] James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, and James Prather. The robots are coming: Exploring the implications of OpenAI codex on introductory programming. In Australasian Computing Education Conference, pages 10–19, 2022.\n",
            "\n",
            "[10] Benoît Frénay and Ata Kabán. A comprehensive introduction to label noise. In European Symposium on\n",
            "\n",
            "Artificial Neural Networks, Computational Intelligence and Machine Learning, 2014.\n",
            "\n",
            "[11] Benoît Frénay and Michel Verleysen. Classification in the presence of label noise: A survey. IEEE\n",
            "\n",
            "Transactions on Neural Networks and Learning Systems, 25(5):845–869, 2013.\n",
            "\n",
            "10\n",
            "\n",
            "[12] Arpit Garg, Cuong Nguyen, Rafael Felix, Thanh-Toan Do, and Gustavo Carneiro. Instance-dependent noisy label learning via graphical modelling. In Winter Conference on Applications of Computer Vision, pages 2288–2298, 2023.\n",
            "\n",
            "[13] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems, volume 31, 2018.\n",
            "\n",
            "[14] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In International Conference on Machine Learning, pages 2304–2313. PMLR, 2018.\n",
            "\n",
            "[15] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In International Conference on Machine Learning, pages 4804–4815. PMLR, 2020.\n",
            "\n",
            "[16] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020.\n",
            "\n",
            "[17] Taehyeon Kim, Jongwoo Ko, JinHwan Choi, and Se-Young Yun. FINE samples for learning with noisy\n",
            "\n",
            "labels. In Advances in Neural Information Processing Systems, volume 34, 2021.\n",
            "\n",
            "[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical\n",
            "\n",
            "report, University of Toronto, 2009.\n",
            "\n",
            "[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional\n",
            "\n",
            "neural networks. In Advances in Neural Information Processing Systems, volume 25, 2012.\n",
            "\n",
            "[20] Junnan Li, Richard Socher, and Steven CH Hoi. DivideMix: Learning with noisy labels as semi-supervised\n",
            "\n",
            "learning. In International Conference on Learning Representations, 2020.\n",
            "\n",
            "[21] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. WebVision Database: Visual learning\n",
            "\n",
            "and understanding from web data. CoRR, 2017.\n",
            "\n",
            "[22] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regular- ization prevents memorization of noisy labels. In Advances in Neural Information Processing Systems, volume 33, pages 20331–20342, 2020.\n",
            "\n",
            "[23] Yang Liu. Identifiability of label noise transition matrix. arXiv preprint arXiv:2202.02016, 2022.\n",
            "\n",
            "[24] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In International Conference on Machine Learning, pages 6543–6553. PMLR, 2020.\n",
            "\n",
            "[25] Eran Malach and Shai Shalev-Shwartz. Decoupling “when to update” from “how to update”. In Advances\n",
            "\n",
            "in Neural Information Processing Systems, volume 30, 2017.\n",
            "\n",
            "[26] Geoffrey J. McLachlan and David Peel. Finite mixture models. Wiley, 1996.\n",
            "\n",
            "[27] Radford M Neal and Geoffrey E Hinton. A view of the EM algorithm that justifies incremental, sparse,\n",
            "\n",
            "and other variants. Learning in graphical models, pages 355–368, 1998.\n",
            "\n",
            "[28] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in\n",
            "\n",
            "deep learning. In Advances in Neural Information Processing Systems, volume 30, 2017.\n",
            "\n",
            "[29] Kento Nishi, Yi Ding, Alex Rich, and Tobias Hollerer. Augmentation strategies for learning with noisy\n",
            "\n",
            "labels. In Conference on Computer Vision and Pattern Recognition, pages 8022–8031, 2021.\n",
            "\n",
            "[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.\n",
            "\n",
            "[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.\n",
            "\n",
            "11\n",
            "\n",
            "[32] Fahad Shamshad, Salman Khan, Syed Waqas Zamir, Muhammad Haris Khan, Munawar Hayat, Fahad Shah- baz Khan, and Huazhu Fu. Transformers in medical imaging: A survey. Medical Image Analysis, page 102802, 2023.\n",
            "\n",
            "[33] Brandon Smart and Gustavo Carneiro. Bootstrapping the relationship between images and their clean and noisy labels. In Winter Conference on Applications of Computer Vision, pages 5344–5354, 2023.\n",
            "\n",
            "[34] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n",
            "\n",
            "[35] David M. Titterington. On the likelihood of mixtures of distributions. Biometrika, 71(3):511–522, 1984.\n",
            "\n",
            "doi: 10.1093/biomet/71.3.511.\n",
            "\n",
            "[36] Immanuel Trummer. From BERT to GPT-3 codex: harnessing the potential of very large language models\n",
            "\n",
            "for data management. VLDB Endowment, 15(12):3770–3773, 2022.\n",
            "\n",
            "[37] Huy Tu and Tim Menzies. Debtfree: minimizing labeling cost in self-admitted technical debt identification\n",
            "\n",
            "using semi-supervised learning. Empirical Software Engineering, 27(4):80, 2022.\n",
            "\n",
            "[38] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. In Advances in Neural Information Processing Systems, volume 33, pages 7597–7610, 2020.\n",
            "\n",
            "[39] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Conference on Computer Vision and Pattern Recognition, pages 2691–2699, 2015.\n",
            "\n",
            "[40] Youjiang Xu, Linchao Zhu, Lu Jiang, and Yi Yang. Faster meta update strategy for noise-robust deep learning. In Conference on Computer Vision and Pattern Recognition, pages 144–153, June 2021.\n",
            "\n",
            "[41] Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. Unsupervised image-to-image translation with generative prior. In Conference on Computer Vision and Pattern Recognition, pages 18332–18341, 2022.\n",
            "\n",
            "[42] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating instance- dependent bayes-label transition matrix using a deep neural network. In International Conference on Machine Learning, pages 25302–25312. PMLR, 2022.\n",
            "\n",
            "[43] Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit memorization effect in learning with noisy labels. In International Conference on Machine Learning, pages 10789–10798. PMLR, 2020.\n",
            "\n",
            "[44] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi Sugiyama. Dual T: Reducing estimation error for transition matrix in label-noise learning. In Advances in Neural Information Processing Systems, volume 33, pages 7260–7271, 2020.\n",
            "\n",
            "[45] Yu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instance-dependent label- noise learning under a structural causal model. In Advances in Neural Information Processing Systems, volume 34, 2021.\n",
            "\n",
            "[46] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.\n",
            "\n",
            "[47] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep\n",
            "\n",
            "learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.\n",
            "\n",
            "[48] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\n",
            "\n",
            "minimization. In International Conference on Learning Representations, 2017.\n",
            "\n",
            "[49] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy labels via total variation regularization. In International Conference on Machine Learning, pages 12501–12512. PMLR, 2021.\n",
            "\n",
            "[50] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with\n",
            "\n",
            "noisy labels. Advances in Neural Information Processing Systems, 31, 2018.\n",
            "\n",
            "[51] Ganlong Zhao, Guanbin Li, Yipeng Qin, Feng Liu, and Yizhou Yu. Centrality and consistency: two-stage clean samples identification for learning with instance-dependent noisy labels. In European Conference on Computer Vision, pages 21–37. Springer, 2022.\n",
            "\n",
            "12\n",
            "\n",
            "[52] Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex M Bronstein, and Or Litany. Contrast to divide: Self-supervised pre-training for learning with noisy labels. In Winter Conference on Applications of Computer Vision, pages 1657–1667, 2022.\n",
            "\n",
            "[53] Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-dependent label noise. In Conference on Computer Vision and Pattern Recognition, pages 10113–10123, June 2021.\n",
            "\n",
            "13\n",
            "\n",
            "A Supplementary Material\n",
            "\n",
            "A.1 Datasets Descriptions\n",
            "\n",
            "We follow the existing literature to generate the IDN labels [38] for CIFAR100 [18]. The dataset contains 50,000 training images and 10,000 testing images of shape 32×32× 3. The dataset is class- balanced with 100 different categories and the IDN rates considered are 0.2,0.3,0.4 and 0.5 [38]. Another important IDN benchmark dataset is the red mini-ImageNet, a real-world dataset from In this dataset, 100 distinct classes, with each class containing 600 images, are CNWL [15]. sampled from ImageNet [31]. The images and their corresponding noisy-labels have been crawled from the internet at various controllable label noise rates. For a fair comparison with the existing literature [40, 12, 6], we have resized the images to 32 × 32 pixels from the 84 × 84 original pixel settings. The noise rate varies from 0% to 80%, but we only show results with 40%,60% and 80% noise rates. Clothing1M [39] is a real-world clothing classification dataset that contains 1 million images with an estimated noise rate of 38.5%. The dataset contains 14 different categories, and the labels are generated from surrounding texts. Images in this dataset are of different sizes, and we follow the resize structure suggested in [20, 17]. This dataset also contains 50k manually validated clean training images, 14k images for the validation set, and 10k testing images. We have not used the clean training, validation set, or any extra training images while training. Only the testing set is used for evaluation. Mini-WebVision [20] contains 65,944 images with their respective labels from the 50 different initial categories from the WebVision [21]. All images are of size 256 × 256 pixels. Following the evaluation process commonly used in this benchmark, 50 categories from the ILSVRC12 [19] dataset are also used for testing.\n",
            "\n",
            "A.2 Extended Implementation Details\n",
            "\n",
            "For our proposed graphical model Fig. 1b, we use the baseline classifier in the graphical model for the term p(y|x;θy), so the architecture of this classifier is kept the same for this term. For the term q(y|x, ˆy) and p(ˆy|x,y;θˆy,ϵ), we use the similar multi-layer perceptron classifier. The training uses stochastic gradient descent (SGD) with a momentum of 0.9. The classifiers’ learning rate is kept the same as their baseline model. The noise rate ϵ is learned using the learnable parameter of the sigmoid activation function, where training uses SGD with a learning rate of 0.001 and momentum of 0.9. The WarmUp stage also follows the baselines and values are 30 for CIFAR100 [18] and red mini-ImageNet [15], 1 for Clothing1M [39], and 5 for mini-WebVision [20]. The batch sizes used are 64 for CIFAR100 [18] and red mini-ImageNet [15], 32 for Clothing1M [39] and mini- WebVision [20]. Additionally, for the self-supervision variant of red mini-ImageNet [15], we use DINO [3], where all the settings of DINO [3] are unchanged from its original work. DINO is trained on red mini-ImageNet [15] and the WarmUp stage is reduced to 10 epochs. For Clothing1M [39], pre-trained ResNet-50 is used for DivideMix [20] and CC [51], and “clean data is not used while training”. Similarly, for SSR [8] on Clothing1M [39], the variant without class balance is used, and no pre-trained network is used. Moreover, while training C2D [52] on mini-WebVision [20], we use the provided pre-trained classifier ResNet-50 with SimCLR [16] for self-supervision.\n",
            "\n",
            "Table 6: Test accuracy (%) of baseline SSR [8] and our approach with SSR [8] on CIFAR100 [18] at 0.5 IDN [38]. Both results are locally reproduced by us and we also show our approach’s final estimated noise rate.\n",
            "\n",
            "Method\n",
            "\n",
            "Test Accuracy Noise Estimation\n",
            "\n",
            "SSR [8] SSR-Ours\n",
            "\n",
            "75.8 76.9\n",
            "\n",
            "0.47\n",
            "\n",
            "A.3 Empirical Analysis of our Approach\n",
            "\n",
            "In this section, we compare our approach with baseline DivideMix [20] (Fig. 4) and SSR [8] (Fig. 5) on CIFAR100 [18] at 0.5 IDN [38]. We show the results on the last 50 training epochs on various metrics, including F1 score, precision, and the ratio of clean classified samples.\n",
            "\n",
            "14\n",
            "\n",
            "e r o c s - 1 F\n",
            "\n",
            "e r o c s - 1 F\n",
            "\n",
            "0.9\n",
            "\n",
            "0.85\n",
            "\n",
            "0.8\n",
            "\n",
            "n o i s i c e r P\n",
            "\n",
            "0.9\n",
            "\n",
            "0.8\n",
            "\n",
            "0.7\n",
            "\n",
            "a t a d n a e l c\n",
            "\n",
            "f o\n",
            "\n",
            "o i t a R\n",
            "\n",
            "0.6\n",
            "\n",
            "0.5\n",
            "\n",
            "0.4\n",
            "\n",
            "ideal ratio\n",
            "\n",
            "Small loss\n",
            "\n",
            "Ours\n",
            "\n",
            "260\n",
            "\n",
            "280\n",
            "\n",
            "300\n",
            "\n",
            "260\n",
            "\n",
            "280\n",
            "\n",
            "300\n",
            "\n",
            "260\n",
            "\n",
            "280\n",
            "\n",
            "300\n",
            "\n",
            "№ of epochs\n",
            "\n",
            "№ of epochs\n",
            "\n",
            "№ of epochs\n",
            "\n",
            "(a) F1 Score\n",
            "\n",
            "(b) Precision\n",
            "\n",
            "(c) Clean Classified Ratio\n",
            "\n",
            "Figure 4: The graphs above show (a) F1-score, (b) precision, and (c) ratio of data classified as clean as a function of the last 50 epochs for our approach (orange) with small loss (with DivideMix [20]) and small loss alone (blue), where the dataset used is CIFAR-100 [18] with 0.5 IDN [38] noise rate.\n",
            "\n",
            "0.94\n",
            "\n",
            "0.93\n",
            "\n",
            "n o i s i c e r P\n",
            "\n",
            "0.97\n",
            "\n",
            "0.96\n",
            "\n",
            "a t a d n a e l c\n",
            "\n",
            "f o\n",
            "\n",
            "0.6\n",
            "\n",
            "SSR Ours\n",
            "\n",
            "0.92\n",
            "\n",
            "o i t a R\n",
            "\n",
            "0.5\n",
            "\n",
            "ideal ratio\n",
            "\n",
            "260\n",
            "\n",
            "280\n",
            "\n",
            "300\n",
            "\n",
            "260\n",
            "\n",
            "280\n",
            "\n",
            "300\n",
            "\n",
            "260\n",
            "\n",
            "280\n",
            "\n",
            "300\n",
            "\n",
            "№ of epochs\n",
            "\n",
            "№ of epochs\n",
            "\n",
            "№ of epochs\n",
            "\n",
            "(a) F1 Score\n",
            "\n",
            "(b) Precision\n",
            "\n",
            "(c) Clean classified ratio\n",
            "\n",
            "Figure 5: The graphs above show (a) F1-score, (b) precision, and (c) ratio of data classified as clean as a function of the last 50 epochs for our approach (orange) with SSR [8] and SSR [8] alone (blue), where the dataset used is CIFAR-100 [18] with 0.5 IDN [38] noise rate.\n",
            "\n",
            "F1-Score: The first metric is F1-score, where Fig. 4a shows the baseline small loss hypothesis [20] results in ≈ 0.70, whilst our approach with small loss [20] reaches around 0.92. On the other hand in SSR [8], Our approach with SSR in Fig. 5a depicts around 0.93 whereas the baseline shows 0.94 F1 scores. Note that our approach shows competitive results for both baselines.\n",
            "\n",
            "Precision: Moreover, Fig. 4b shows the precision comparison where the small loss [20] is around 0.65, whereas our approach with small loss [20] produces a result around 0.95 − 0.96. Addition- ally, Fig. 5b shows that our approach has a precision of 0.97 that is slightly larger than the baseline’s precision of around 0.95 − 0.96. This shows the efficacy of our approach in selecting positive instances.\n",
            "\n",
            "Ratio of samples classified as clean: Additionally, data in Figure Fig. 4c exhibits the proportion of instances identified as clean. This setting employs a noise rate of 0.5. Consequently, in an optimal scenario, the approach should theoretically designate half of the total samples as clean (ideal case is 0.5). Our approach with small loss [20] shows around 0.53 in comparison to small loss [20] where the results are around 0.65−0.70. Also, our approach with SSR [8] shows 0.53 (Fig. 5c) as compared to baseline SSR [8] which is around 0.65 − 0.70. Our approach is much closer to the ideal case using both baselines.\n",
            "\n",
            "Table 6 shows the results of SSR [8] on CIFAR100 [18] at 0.5 IDN [38]. The results are reproduced by us, including baseline SSR [8] and our approach with SSR [8]. Estimating noise rate and integrating it with SSR [8] boosts model performance.\n",
            "\n",
            "These empirical comparisons show the effectiveness and the need for the noise rate estimation as it helps to provide a better curriculum for the sampling of clean-label training samples during training, which is crucial for enabling a competitive performance in noise-label learning benchmarks.\n",
            "\n",
            "15\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredPDFLoader('/content/arabic.pdf')#for tables\n",
        "arabic = loader.load()"
      ],
      "metadata": {
        "id": "KW9zBncZ4sTC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(arabic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1sYlahO7hqP",
        "outputId": "39cf3d7e-cc35-4367-fd78-788c159a35c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arabic[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "5khmeM1W7kmQ",
        "outputId": "785ca92f-b7fc-4ca4-c978-c1a9ead94f08"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'١p”Ím¿ÀÖm¿(cid:143)ƒÌÑm¿(cid:143)ƒÛÍmÈ×ّ¾m¿Ø~ØŠm£¹×ÓØmƒ‹Ým¿(cid:141)mu\\n\\n1\\n\\nThis sample shows a mixture of Arabic and English. Arabic is the main script. There are running heads, too.\\n\\n\\\\bodydir apparently can be used only in the preamble of the doc- ument. \\\\textdir should reverse \\\\hboxes with TLT, but for some reason in Aleph does not work (Omega reverses hboxes correctly). This is important in lists and sections. Currently, the \\\\bodydir is that of the main language (the last one in the package options), which means the document has a ﬁxed TLT layout.\\n\\nHello\\n\\nÈÚÌ¼Ñ³ÛÖ õm¿(cid:141)mum×mÈƒØm¾pÌnÚ‘mŠ“q¹m×mÏw¸n¾Èëì vœØّŽ»{(cid:143)sm×v¬‹ّŠm×m‡wÉ²ëì õm¿¬qnŽs×mÈ—nŽspÁÓØp¼Á Ï´•mÈË(cid:143)×Èëì m«wqnŽmƒ‹Ýmß¬ÐÞËq(cid:143)ّ`«Ñ»ÁّËn“ØÝŒmvÖË¤À¸n×ÓØm¿ÀّÖ“q„nÏÖ×v¬n(cid:176)õ×¿Û• —Þ`p„¸Û¸tm¿˜ÛkÛt“ØmÒ.\\n\\np”Ím¿ÀÖm¿(cid:143)ƒÌÑm¿(cid:143)ƒÛÍmÈ×ّ¾m¿Ø~ØŠm£¹×ÓØmƒ‹Ým¿(cid:141)mu\\n\\nõ\\n\\nõm¿´(cid:143)ž×mÈË¼nÎ×mÈ«wqnŽ×Èëì\\n\\nA list, with random item breaks:\\n\\nÈÚÌ¼Ñ³ÛÖ\\n\\nHello\\n\\n١.p”Ím¿ÀÖm¿(cid:143)ƒÌÑm¿(cid:143)ƒÛÍmÈ×ّ¾m¿Ø~ØŠm£¹×ÓØmƒ‹Ým¿(cid:141)mu\\n\\nõm¿(cid:141)mu\\n\\nvœØّŽ»{(cid:143)sm×v¬‹ّŠm×m‡wÉ²ëì\\n\\nõm¿´(cid:143)ž×mÈË¼nÎ\\n\\nm×(m×mÈƒØm¾pÌnÚ‘mŠ“q¹m×mÏw¸n¾Èëì\\n\\nõÏ´•mÈË(cid:143)×Èëì õm¿¬qnŽs×mÈ—nŽspÁÓØp¼Ám«wqnŽmƒ‹Ý\\n\\n×mÈ«wqnŽ×Èëì\\n\\n(mß¬ÐÞËq(cid:143)ّ`«Ñ»ÁّËn“ØÝŒmvÖË¤À¸n×ÓØm¿ÀّÖ“q„nÏÖ×v¬n(cid:176)õ×¿Û•—Þ\\n\\nThird\\n\\n`p„¸Û¸tm¿˜ÛkÛt“ØmÒ.\\n\\nA section follows:\\n\\n١p”Ím¿ÀÖm¿(cid:143)ƒÌÑm¿(cid:143)ƒÛÍmÈ×ّ¾m¿Ø~ØŠm£¹×ÓØmƒ‹Ý\\n\\nm¿(cid:141)mu\\n\\nBlah. English text in an arabic context. Note it is indented still at the right, as\\n\\nit must be.\\n\\n٢p”Ím¿ÀÖm¿(cid:143)ƒÌÑm¿(cid:143)ƒÛÍmÈ×ّ¾m¿Ø~ØŠm£¹\\n\\nNow an English paragraph. Arabic paragraph as a quote:\\n\\n3 ATITLEINTHEENGLISHPARTTOSEEWHATHAPPENSANDIFTHEFORMATESCORRECT\\n\\nHello p”Ím¿ÀÖm¿(cid:143)ƒÌÑm¿(cid:143)ƒÛÍmÈ×ّ¾m¿Ø~ØŠm£¹×ÓØmƒ‹Ým¿(cid:141)mu ÈÚÌ¼Ñ³ÛÖvœØّŽ»{(cid:143)sm×v¬‹ّŠm×m‡wÉ²ëì õm¿(cid:141)mum×mÈƒØm¾pÌnÚ‘mŠ “q¹m×mÏw¸n¾Èëì õm¿´(cid:143)ž×mÈË¼nÎ×mÈ«wqnŽ× õm¿¬qnŽs×mÈ—nŽspÁÓØp¼Ám«wqnŽmƒ‹Ýmß¬ÐÞËq(cid:143)ّ`«Ñ»ÁّËn Èëì “ØÝŒmvÖË¤À¸n×ÓØm¿ÀّÖ“q„nÏÖ×v¬n(cid:176)õ×¿Û•—Þ`p„¸Û¸tm¿˜ÛkÛt “ØmÒ.\\n\\nõÏ´•mÈË(cid:143)×Èëì\\n\\n3A title in the English part to see what happens and\\n\\nif the format es correct\\n\\nBy the way, page numbers are more or less correct (but still to be improved).\\n\\nNote the section in latin script has a wrong format.\\n\\nBlah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah. Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah. Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah Blah.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOJTJqmY7-aP",
        "outputId": "4eb888ec-1327-4bdd-dca4-92f703c1464c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (20240706)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PDFMinerLoader #arabic"
      ],
      "metadata": {
        "id": "fCmpcfRo7mIw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PDFMinerLoader(\"/content/arabic-1333488651.pdf\")"
      ],
      "metadata": {
        "id": "YqQr3eTJ822e"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arabic = loader.load()"
      ],
      "metadata": {
        "id": "3WIT0kfe89Y-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arabic[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV7U0q829Asu",
        "outputId": "92e1b62a-2cd0-4203-804d-66cc19b2acf0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'title': 'SAMPLE SHORT FORM WRITTEN CONSENT DOCUMENT', 'creator': 'Acrobat PDFMaker 5.0 for Word', 'moddate': '2015-10-04T20:01:05+02:00', 'producer': 'Acrobat Distiller 5.0.5 (Windows)', 'author': 'IRB', 'creationdate': '2002-10-07T09:38:34-05:00', 'total_pages': 2, 'source': '/content/arabic-1333488651.pdf'}, page_content='ﻲﺳارد ﺚﺤﺑ ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻰﻠﻋ ﺔﻘﻓاﻮﻣ\\n\\n. \\n\\nﻲﻤﻠﻋ ﺚﺤﺑ ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻚﻴﻠﻋ حﺮﺘﻘﻧ\\n\\n : \\n\\n؛ﺔﺒﻠﻄﺘﻤﻟا ﺔّﻴﻨﻡﺰﻟا ةﺮﺘﻔﻟاو تاءاﺮﺟﻹاو فاﺪهﻷﺎﺑ \\n؛ﺔّﻴﻘﻴﺒﻄﺕ تاءاﺮﺟإ ﺔّﻱﺄﺑ \\nا ﻦﻋ ﺔﺠﺕﺎﻧ ﻊﻓﺎﻨﻡ وأ تﺎﻘﻱﺎﻀﻡ يﺄﺑو ﺔﻌﻗﻮﺘﻡو ﺔﻨﻜﻤﻡ رﺎﻄﺧأ ﺔّﻱﺄﺑ \\n؛ﺔﻠﻤﺘﺤﻡو ﺔﻠﻱﺪﺑ تﺎﺟﻼﻋ وأ تاءاﺮﺟإ ﺔّﻱأ \\nتﺎﻡﻮﻠﻌﻤﻟا ﺔّﻱﺮﺳ ﻰﻠﻋ ءﺎﻘﺒﻟا ﺔﻴﻔﻴآ \\n\\nﻚﻤﻠﻌُﻱ نأ ﺚﺡﺎﺒﻟا ﻦﻡ ﺐﻠﻄﺘﻱ ﻚﺘﻘﻓاﻮﻡ ﻞﺒﻗ\\n(i)\\n(ii)\\n(iii)\\n(iv)\\n(v)\\n\\n و \\n\\n. \\n\\n؛ﺚﺤﺒﻟ\\n\\n؛ﺎﻡ رﺮﺽ وأ ىذأ ثوﺪﺡ لﺎﺡ ﻲﻓ ﻚﻟ ﻦﻡﺆﻡ ﻲﺒﻃ جﻼﻋ وأ تﺎﻀﻱﻮﻌﺕ ﺔّﻱأ \\n؛ﺔﻌﻗﻮﺘﻡ ﺮﻴﻏ ةرﻮﻄﺧ لﺎﻤﺘﺡا \\nﻦﻜﻤﻡ ﻲﺘﻟا تﻻﺎﺤﻟا \\n؛ﻪﻟﺬﺒﺕ نأ ﻦﻜﻤﻡ ﻲﻓﺎﺽإ ﺪﻬﺟ يأ \\n؟ﺚﺤﺒﻟا ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻦﻋ ﻒﻗﻮﺘﺕ نأ ترﺮﻗ لﺎﺡ ﻲﻓ ﻞﺼﺤﻱ اذﺎﻡ \\n؟ﺚﺤﺒﻟا ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻲﻓ ﻚﺘﻤﻱﺰﻋ ﻰﻠﻋ ﺮﺛﺆﺕ نأ ﻦﻜﻤﻡ ةﺪﻱﺪﺟ تﺎﺟﺎﺘﻨﺘﺳﺎﺑ ﻚﻡﻼﻋإ ﺐﺟﻮﺘﻱ ﻰﺘﻡ \\nﺚﺤﺒﻟا ﻲﻓ نﻮآﺮﺘﺸﻴﺳ ﺎﺼﺨﺵ ﻢآو \\n\\n ﻦﻋ ﺎﻀﻱأ ﻚﻤﻠﻌُﻱ نأ ﺚﺡﺎﺒﻟا ﻰﻠﻋ ﺐﺠﻱ ، ةروﺮﻀﻟا ﺪﻨﻋ\\n(i)\\n(ii)\\n(iii)\\n(iv)\\n(v)\\n(vi)\\n (vii)\\n\\n؛ﺚﺤﺒﻟا ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻦﻋ ﻚﻓﺎﻘﻱإ ﻰﻠﻋ ﺚﺡﺎﺒﻟا ﺚﺤﺕ نأ \\n\\n. \\n\\n ﻦﻋ ﺔّﻴﻄﺧ ﺔﺹﻼﺧو ﺪﻨﺘﺴﻤﻟا اﺬه ﻦﻋ ﺔﻌﻗﻮﻡ ﺔﺨﺴﻧ ﻚﺋﺎﻄﻋإ ﺐﺟﻮﺘﻤﻟا ﻦﻤﻓ كرﺎﺸﺕ نأ ترﺮﻗ اذإ\\nﺚﺤﺒﻟا\\n\\n. \\n\\n ـﺑ تﺎﻗوﻷا ﻦﻡ ﺖﻗو يأ ﻲﻓ لﺎﺼﺕﻻا ﻦﻜﻤﻤﻟا ﻦﻤﻓ ﺚﺤﺒﻟا اﺬه ﻦﻋ ﺎﻡ ﺔﻠﺌﺳأ ﻚﻱﺪﻟ نﺎآ اذإ\\n ﻢﻗﺮﻟا ﻰﻠﻋ\\n _______________________\\n\\n _________________\\n\\n. \\n\\n لﺎﺡ ﻲﻓ ﻪﻠﻤﻋ ﻚﻴﻠﻋ ﺐﺟﻮﺘﻱ ﺎﻤﻴﻓ وأ ﺚﺤﺒﻟا اﺬه ﻲﻓ كرﺎﺸﻤآ ﻚﻗﻮﻘ\\n\\n_______\\n\\n___\\n\\n_______\\n\\nـﺑ تﺎﻗوﻷا ﻦﻡ ﺖﻗو يأ ﻲﻓ لﺎﺼﺕﻻا ﻦﻜﻤﻤﻟا ﻦﻤﻓ\\n\\nﺤﺑ ﻖﻠﻌﺘﺕ ﺎﻡ ﺔﻠﺌﺳأ ﻚﻱﺪﻟ نﺎآ اذإ\\nرﺮﻀﺑ وأ ىذﺄﺑ ﻚﺘﺑﺎﺹإ\\n ﻢﻗﺮﻟا ﻰﻠﻋ\\n\\n. \\n\\n _________________\\n\\nوأ ﺔآرﺎﺸﻤﻟا مﺪﻋ ترﺮﻗ لﺎﺡ ﻲﻓ ﻊﻓﺎﻨﻡ ﺔﻱأ ﺮﺴﺨﺕ ﻦﻟو ﺐﻗﺎﻌُﺕ ﻦﻟ\\n\\n .\\n\\nﺔّﻱرﺎﻴﺘﺧا ﺚﺤﺒﻟا اﺬه ﻲﻓ ﻚﺘآرﺎﺸﻡ\\nﺎﻡ ﺖﻗو يأ ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻦﻋ ﻒﻗﻮﺘﻟا\\n\\n . \\n\\nShort Form Consent - ARABIC\\n\\x0cنأو ﺚﺤﺒﻟا اﺬه ﻲﻓ ﺔآرﺎﺸﻤﻟا ﻰﻠﻋ ﺎﻱرﺎﻴﺘﺧا ﻖﻓاﻮﺕ ﻚﻧﺄﺑ ﺮﻘﺕ ﺪﻨﺘﺴﻤﻟا اﺬه ﻰﻠﻋ ﻚﺋﺎﻀﻡإ دﺮﺠﻤﺑ\\n  .\\nﺎﻴﻬﻔﺵ ﻚﻟ ﺖﺡﺮﺵ ﺪﻗ ﻩﻼﻋأ ﺔﻧوﺪﻤﻟا تﺎﻡﻮﻠﻌﻤﻟا\\n\\n_____________________________ \\nكرﺎﺸﻤﻟا ءﺎﻀﻡإ  \\n\\n________________ \\nﺦﻱرﺎﺘﻟا \\n\\n_____________________________ \\nﺪهﺎﺸﻟا ءﺎﻀﻡإ \\n\\n________________ \\nﺦﻱرﺎﺘﻟا \\n\\nShort Form Consent - ARABIC')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qp_fB_iD9EX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}